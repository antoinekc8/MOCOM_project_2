{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0149d6b9",
   "metadata": {},
   "source": [
    "# RL Agent traffic light control (SUMO + TraCI + PyTorch)\n",
    "\n",
    "This notebook trains and evaluates an **RL agent** for traffic control using a simple neural network.\n",
    "\n",
    "## Algorithm Overview\n",
    "\n",
    "The RL Agent learning approach:\n",
    "1. **Brain (Neural Network)**: A small PyTorch network that takes queue state (4 inputs) and outputs action probabilities (2 actions)\n",
    "2. **State**: Normalized incoming vehicle counts from 4 directions (N, S, E, W)\n",
    "3. **Action**: Binary choice - phase 0 (N-S green) or phase 2 (E-W green)\n",
    "4. **Reward**: Negative of average waiting time (lower waiting = higher reward)\n",
    "5. **Safety**: Yellow phase enforced between transitions\n",
    "\n",
    "## Workflow\n",
    "1. **Training Phase**: Learn policy from trial-and-error over multiple episodes\n",
    "2. **Evaluation Phase**: Test trained agent on fresh scenario\n",
    "3. **KPI Analysis**: Report metrics compared to baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d9457ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TRAINING CONFIGURATION ---\n",
      "Device: cpu\n",
      "Target Time: ~1 Hour\n",
      "Episodes: 500\n",
      "Steps per Episode: 900\n",
      "Final Model Path: c:\\Users\\antoi\\OneDrive\\Documents\\Documents\\Devoirs\\Études sup\\ENTPE 3A\\Majeure transports\\Mobility Control and Management\\Project\\Livrable 2\\MOCOM_project_2\\Network with RL control\\models\\final_model.pth\n",
      "Model exists: True\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION (OPTIMIZED FOR 1 HOUR)\n",
    "# ==========================================\n",
    "# Paths\n",
    "BASE_DIR = os.getcwd()\n",
    "SCENARIO_DIR = os.path.join(BASE_DIR, \"Network with RL control\")\n",
    "RESULTS_DIR = os.path.join(SCENARIO_DIR, \"results\")\n",
    "MODEL_DIR = os.path.join(SCENARIO_DIR, \"models\")\n",
    "CONFIG_PATH = os.path.join(SCENARIO_DIR, \"ff_heterogeneous.sumocfg\")\n",
    "FINAL_MODEL_PATH = os.path.join(MODEL_DIR, \"final_model.pth\")\n",
    "\n",
    "# Hyperparameters\n",
    "NUM_EPISODES = 500          # Fits within 1 hour\n",
    "MAX_STEPS = 900             # 15 minutes simulated per episode (enough to learn congestion)\n",
    "BATCH_SIZE = 32             # Faster processing\n",
    "MEMORY_SIZE = 20000\n",
    "GAMMA = 0.95                # Discount factor\n",
    "LEARNING_RATE = 0.002       # Slightly higher learning rate for faster convergence\n",
    "\n",
    "# Exploration Settings\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.01\n",
    "EPSILON_DECAY = 0.99        # Reaches minimal exploration around episode 300\n",
    "\n",
    "# Traffic Settings\n",
    "TLS_IDS = [\"E1\", \"E2\", \"E3\", \"E4\"]\n",
    "YELLOW_DURATION = 3\n",
    "MIN_GREEN = 10\n",
    "\n",
    "# Hardware Check\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"--- TRAINING CONFIGURATION ---\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Target Time: ~1 Hour\")\n",
    "print(f\"Episodes: {NUM_EPISODES}\")\n",
    "print(f\"Steps per Episode: {MAX_STEPS}\")\n",
    "print(f\"Final Model Path: {FINAL_MODEL_PATH}\")\n",
    "print(f\"Model exists: {os.path.exists(FINAL_MODEL_PATH)}\")\n",
    "print(\"------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f502f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SUMO binary: C:\\Program Files (x86)\\Eclipse\\Sumo\\bin\\sumo-gui.exe\n",
      "\n",
      "======================================================================\n",
      "RL AGENT TRAINING STARTED\n",
      "======================================================================\n",
      "\n",
      "--- Episode 1/5 ---\n",
      "Episode 1 completed: avg_reward = -14.3365\n",
      "\n",
      "--- Episode 2/5 ---\n",
      "Episode 2 completed: avg_reward = -15.1301\n",
      "\n",
      "--- Episode 3/5 ---\n",
      "Episode 3 completed: avg_reward = -13.7541\n",
      "\n",
      "--- Episode 4/5 ---\n",
      "Episode 4 completed: avg_reward = -14.7695\n",
      "\n",
      "--- Episode 5/5 ---\n",
      "Episode 5 completed: avg_reward = -13.5166\n",
      "Model saved to c:\\Users\\antoi\\OneDrive\\Documents\\Documents\\Devoirs\\Études sup\\ENTPE 3A\\Majeure transports\\Mobility Control and Management\\Project\\Livrable 2\\MOCOM_project_2\\Network with RL control\\models\\traffic_agent.pth\n",
      "\n",
      "======================================================================\n",
      "TRAINING COMPLETED\n",
      "======================================================================\n",
      "\n",
      "Training log summary:\n",
      "  Episode 1: avg_reward = -14.3365\n",
      "  Episode 2: avg_reward = -15.1301\n",
      "  Episode 3: avg_reward = -13.7541\n",
      "  Episode 4: avg_reward = -14.7695\n",
      "  Episode 5: avg_reward = -13.5166\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 2. NEURAL NETWORK (DQN)\n",
    "# ==========================================\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DQN, self).__init__()\n",
    "        # Lighter network for faster training on normal PC\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# ==========================================\n",
    "# 3. RL AGENT\n",
    "# ==========================================\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
    "        self.epsilon = EPSILON_START\n",
    "        \n",
    "        self.policy_net = DQN(state_size, action_size).to(device)\n",
    "        self.target_net = DQN(state_size, action_size).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=LEARNING_RATE)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def act(self, state):\n",
    "        if random.random() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.policy_net(state_tensor)\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "    def act_inference(self, state):\n",
    "        \"\"\"Inference mode: no exploration, just use policy\"\"\"\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.policy_net(state_tensor)\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        minibatch = random.sample(self.memory, BATCH_SIZE)\n",
    "        \n",
    "        states = torch.FloatTensor(np.array([m[0] for m in minibatch])).to(device)\n",
    "        actions = torch.LongTensor([m[1] for m in minibatch]).unsqueeze(1).to(device)\n",
    "        rewards = torch.FloatTensor([m[2] for m in minibatch]).to(device)\n",
    "        next_states = torch.FloatTensor(np.array([m[3] for m in minibatch])).to(device)\n",
    "        dones = torch.FloatTensor([m[4] for m in minibatch]).to(device)\n",
    "\n",
    "        curr_Q = self.policy_net(states).gather(1, actions).squeeze(1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_Q = self.target_net(next_states).max(1)[0]\n",
    "            target_Q = rewards + (GAMMA * next_Q * (1 - dones))\n",
    "\n",
    "        loss = self.criterion(curr_Q, target_Q)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save(self.policy_net.state_dict(), path)\n",
    "\n",
    "    def load(self, path):\n",
    "        \"\"\"Load pre-trained model\"\"\"\n",
    "        if os.path.exists(path):\n",
    "            self.policy_net.load_state_dict(torch.load(path, map_location=device))\n",
    "            self.policy_net.eval()\n",
    "            print(f\"✓ Model loaded from {path}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"✗ Model file not found: {path}\")\n",
    "            return False\n",
    "\n",
    "# ==========================================\n",
    "# 4. SUMO HELPER FUNCTIONS\n",
    "# ==========================================\n",
    "def check_sumo_env():\n",
    "    if 'SUMO_HOME' not in os.environ:\n",
    "        sys.exit(\"Error: SUMO_HOME environment variable is not set.\")\n",
    "    tools = os.path.join(os.environ['SUMO_HOME'], 'tools')\n",
    "    sys.path.append(tools)\n",
    "    os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "    \n",
    "    # Ensure config file exists\n",
    "    if not os.path.exists(CONFIG_PATH):\n",
    "        import shutil\n",
    "        print(\"Config file not found, attempting to copy from 'Original network'...\")\n",
    "        original_dir = os.path.join(BASE_DIR, \"Original network\")\n",
    "        for file in [\"ff_heterogeneous.sumocfg\", \"ff.net.xml\", \"ff_heterogeneous.rou.xml\"]:\n",
    "            src = os.path.join(original_dir, file)\n",
    "            dst = os.path.join(SCENARIO_DIR, file)\n",
    "            if os.path.exists(src):\n",
    "                shutil.copy(src, dst)\n",
    "    return tools\n",
    "\n",
    "tools_path = check_sumo_env()\n",
    "import traci\n",
    "\n",
    "def get_sumo_binary():\n",
    "    # Always headless for performance\n",
    "    return os.path.join(os.environ['SUMO_HOME'], 'bin', 'sumo') \n",
    "\n",
    "def get_state(tls_id):\n",
    "    \"\"\"\n",
    "    State (Size 3):\n",
    "    1. Total Queue Length (Normalized)\n",
    "    2. Max Queue Length (Normalized)\n",
    "    3. Current Phase Index (Normalized)\n",
    "    \"\"\"\n",
    "    lanes = traci.trafficlight.getControlledLanes(tls_id)\n",
    "    halting = [traci.lane.getLastStepHaltingNumber(lane) for lane in lanes]\n",
    "    \n",
    "    total_queue = sum(halting) / 50.0  # Normalized\n",
    "    max_queue = max(halting) / 20.0    # Normalized\n",
    "    phase = traci.trafficlight.getPhase(tls_id) / 4.0\n",
    "    \n",
    "    return np.array([total_queue, max_queue, phase], dtype=np.float32)\n",
    "\n",
    "def get_reward(tls_id, waiting_time_prev):\n",
    "    \"\"\"\n",
    "    Reward function CLIPPED to prevent huge numbers.\n",
    "    \"\"\"\n",
    "    lanes = traci.trafficlight.getControlledLanes(tls_id)\n",
    "    current_wait = sum([traci.lane.getWaitingTime(lane) for lane in lanes])\n",
    "    current_queue = sum([traci.lane.getLastStepHaltingNumber(lane) for lane in lanes])\n",
    "    \n",
    "    # Reward = (WaitTime Reduction) - (Queue Penalty)\n",
    "    diff = waiting_time_prev - current_wait\n",
    "    \n",
    "    # Scaling to keep reward small (e.g., between -5 and 5)\n",
    "    reward = (diff * 0.2) - (current_queue * 0.05)\n",
    "    \n",
    "    # CLAMPING: Crucial for stability\n",
    "    reward = max(min(reward, 5.0), -5.0)\n",
    "    \n",
    "    return reward, current_wait\n",
    "\n",
    "# ==========================================\n",
    "# 5. MAIN TRAINING LOOP\n",
    "# ==========================================\n",
    "def train():\n",
    "    # Input: 3, Output: 2\n",
    "    agent = DQNAgent(state_size=3, action_size=2)\n",
    "    sumo_bin = get_sumo_binary()\n",
    "    \n",
    "    print(\"\\nStarting Optimized Training...\")\n",
    "\n",
    "    for episode in range(NUM_EPISODES):\n",
    "        # Start SUMO\n",
    "        sumo_cmd = [sumo_bin, \"-c\", CONFIG_PATH, \"--no-step-log\", \"true\", \"--waiting-time-memory\", \"1000\"]\n",
    "        try:\n",
    "            traci.start(sumo_cmd)\n",
    "        except Exception as e:\n",
    "            print(f\"Error starting SUMO: {e}\")\n",
    "            break\n",
    "\n",
    "        step = 0\n",
    "        total_reward = 0\n",
    "        \n",
    "        # Tracking data for all 4 intersections\n",
    "        tls_data = {t: {\"prev_wait\": 0, \"phase_time\": 0} for t in TLS_IDS}\n",
    "\n",
    "        while step < MAX_STEPS:\n",
    "            traci.simulationStep()\n",
    "            step += 1\n",
    "            \n",
    "            # Make decisions every 5 seconds (not every step)\n",
    "            if step % 5 == 0:\n",
    "                for tls in TLS_IDS:\n",
    "                    # 1. Observe State\n",
    "                    state = get_state(tls)\n",
    "                    \n",
    "                    # 2. Select Action\n",
    "                    action = agent.act(state)\n",
    "                    \n",
    "                    # 3. Apply Action (Logic)\n",
    "                    current_phase = traci.trafficlight.getPhase(tls)\n",
    "                    \n",
    "                    # Phase 0=NS Green, 1=Yellow, 2=EW Green, 3=Yellow\n",
    "                    # Action 0=NS Green, Action 1=EW Green\n",
    "                    \n",
    "                    if current_phase == 1 or current_phase == 3:\n",
    "                        # Yellow phase logic: wait it out\n",
    "                        tls_data[tls][\"phase_time\"] += 5\n",
    "                        if tls_data[tls][\"phase_time\"] >= YELLOW_DURATION:\n",
    "                            next_phase = 2 if current_phase == 1 else 0\n",
    "                            traci.trafficlight.setPhase(tls, next_phase)\n",
    "                            tls_data[tls][\"phase_time\"] = 0\n",
    "                    else:\n",
    "                        # Green phase logic\n",
    "                        target_phase = 0 if action == 0 else 2\n",
    "                        \n",
    "                        if current_phase != target_phase:\n",
    "                            # We want to switch\n",
    "                            if tls_data[tls][\"phase_time\"] >= MIN_GREEN:\n",
    "                                traci.trafficlight.setPhase(tls, current_phase + 1) # Go Yellow\n",
    "                                tls_data[tls][\"phase_time\"] = 0\n",
    "                            else:\n",
    "                                tls_data[tls][\"phase_time\"] += 5 # Keep Green\n",
    "                        else:\n",
    "                            tls_data[tls][\"phase_time\"] += 5 # Keep Green\n",
    "                    \n",
    "                    # 4. Reward & Learn\n",
    "                    reward, new_wait = get_reward(tls, tls_data[tls][\"prev_wait\"])\n",
    "                    \n",
    "                    # Remember\n",
    "                    next_state = get_state(tls)\n",
    "                    agent.remember(state, action, reward, next_state, False)\n",
    "                    \n",
    "                    tls_data[tls][\"prev_wait\"] = new_wait\n",
    "                    total_reward += reward\n",
    "\n",
    "                # Train Agent\n",
    "                agent.replay()\n",
    "\n",
    "        traci.close()\n",
    "        \n",
    "        # Update Target Net\n",
    "        if episode % 5 == 0:\n",
    "            agent.update_target_network()\n",
    "            \n",
    "        # Decay Epsilon\n",
    "        if agent.epsilon > EPSILON_END:\n",
    "            agent.epsilon *= EPSILON_DECAY\n",
    "\n",
    "        print(f\"Ep {episode+1}/{NUM_EPISODES} | Reward: {total_reward:.2f} | Eps: {agent.epsilon:.2f}\")\n",
    "\n",
    "        # Autosave\n",
    "        if (episode + 1) % 25 == 0:\n",
    "            agent.save(os.path.join(MODEL_DIR, f\"model_ep{episode+1}.pth\"))\n",
    "\n",
    "    agent.save(os.path.join(MODEL_DIR, \"final_model.pth\"))\n",
    "    print(\"Training Completed.\")\n",
    "\n",
    "# Note: Uncomment the following line to run training (commented out to use pre-trained model)\n",
    "# if __name__ == \"__main__\":\n",
    "#     train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27118339",
   "metadata": {},
   "source": [
    "## Evaluation Phase\n",
    "\n",
    "Test the trained agent on a fresh scenario and compare against baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ff5f3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Starting RL Agent Evaluation\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "RL AGENT EVALUATION (Using final_model.pth)\n",
      "======================================================================\n",
      "Using device: cpu\n",
      "Model path: c:\\Users\\antoi\\OneDrive\\Documents\\Documents\\Devoirs\\Études sup\\ENTPE 3A\\Majeure transports\\Mobility Control and Management\\Project\\Livrable 2\\MOCOM_project_2\\Network with RL control\\models\\final_model.pth\n",
      "✓ Model loaded from c:\\Users\\antoi\\OneDrive\\Documents\\Documents\\Devoirs\\Études sup\\ENTPE 3A\\Majeure transports\\Mobility Control and Management\\Project\\Livrable 2\\MOCOM_project_2\\Network with RL control\\models\\final_model.pth\n",
      "\n",
      "Starting SUMO evaluation...\n",
      "SUMO started successfully\n",
      "Running evaluation simulation (1 hour)...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 204\u001b[39m\n\u001b[32m    202\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m70\u001b[39m)\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m     success = \u001b[43mevaluate_rl_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m success:\n\u001b[32m    206\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPlease ensure final_model.pth exists before running evaluation.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 140\u001b[39m, in \u001b[36mevaluate_rl_agent\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    138\u001b[39m step_count = \u001b[32m0\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m traci.simulation.getTime() < \u001b[32m3600\u001b[39m:  \u001b[38;5;66;03m# 1 hour\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     \u001b[43mtraci\u001b[49m\u001b[43m.\u001b[49m\u001b[43msimulationStep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m     step_count += \u001b[32m1\u001b[39m\n\u001b[32m    143\u001b[39m     \u001b[38;5;66;03m# Make decisions every 5 seconds\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\main.py:200\u001b[39m, in \u001b[36msimulationStep\u001b[39m\u001b[34m(step)\u001b[39m\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msimulationStep\u001b[39m(step=\u001b[32m0\u001b[39m):\n\u001b[32m    195\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"simulationStep(float) -> None\u001b[39;00m\n\u001b[32m    196\u001b[39m \u001b[33;03m    Make a simulation step and simulate up to the given second in sim time.\u001b[39;00m\n\u001b[32m    197\u001b[39m \u001b[33;03m    If the given value is 0 or absent, exactly one step is performed.\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[33;03m    Values smaller than or equal to the current sim time result in no action.\u001b[39;00m\n\u001b[32m    199\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m     \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msimulationStep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py:370\u001b[39m, in \u001b[36mConnection.simulationStep\u001b[39m\u001b[34m(self, step)\u001b[39m\n\u001b[32m    368\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(step) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mint\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m step >= \u001b[32m1000\u001b[39m:\n\u001b[32m    369\u001b[39m     warnings.warn(\u001b[33m\"\u001b[39m\u001b[33mAPI change now handles step as floating point seconds\u001b[39m\u001b[33m\"\u001b[39m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m370\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sendCmd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCMD_SIMSTEP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m subscriptionResults \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._subscriptionMapping.values():\n\u001b[32m    372\u001b[39m     subscriptionResults.reset()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py:232\u001b[39m, in \u001b[36mConnection._sendCmd\u001b[39m\u001b[34m(self, cmdID, varID, objID, format, *values)\u001b[39m\n\u001b[32m    230\u001b[39m     \u001b[38;5;28mself\u001b[39m._string += struct.pack(\u001b[33m\"\u001b[39m\u001b[33m!i\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(objID)) + objID\n\u001b[32m    231\u001b[39m \u001b[38;5;28mself\u001b[39m._string += packed\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sendExact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py:131\u001b[39m, in \u001b[36mConnection._sendExact\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    129\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33msending\u001b[39m\u001b[33m\"\u001b[39m, Storage(length + \u001b[38;5;28mself\u001b[39m._string).getDebugString())\n\u001b[32m    130\u001b[39m \u001b[38;5;28mself\u001b[39m._socket.send(length + \u001b[38;5;28mself\u001b[39m._string)\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_recvExact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _DEBUG:\n\u001b[32m    133\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mreceiving\u001b[39m\u001b[33m\"\u001b[39m, result.getDebugString())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py:109\u001b[39m, in \u001b[36mConnection._recvExact\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    107\u001b[39m result = \u001b[38;5;28mbytes\u001b[39m()\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) < \u001b[32m4\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     t = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_socket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t:\n\u001b[32m    111\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# EVALUATION: Run trained agent using final_model.pth\n",
    "# ==========================================\n",
    "\n",
    "import time\n",
    "\n",
    "# Import traci for SUMO interaction\n",
    "if 'SUMO_HOME' in os.environ:\n",
    "    tools = os.path.join(os.environ['SUMO_HOME'], 'tools')\n",
    "    if tools not in sys.path:\n",
    "        sys.path.append(tools)\n",
    "import traci\n",
    "\n",
    "# ==========================================\n",
    "# REQUIRED CLASSES (ensure DQN is available)\n",
    "# ==========================================\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
    "        self.epsilon = EPSILON_START\n",
    "        \n",
    "        self.policy_net = DQN(state_size, action_size).to(device)\n",
    "        self.target_net = DQN(state_size, action_size).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=LEARNING_RATE)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def act_inference(self, state):\n",
    "        \"\"\"Inference mode: no exploration, just use policy\"\"\"\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.policy_net(state_tensor)\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "    def load(self, path):\n",
    "        \"\"\"Load pre-trained model\"\"\"\n",
    "        if os.path.exists(path):\n",
    "            self.policy_net.load_state_dict(torch.load(path, map_location=device))\n",
    "            self.policy_net.eval()\n",
    "            print(f\"✓ Model loaded from {path}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"✗ Model file not found: {path}\")\n",
    "            return False\n",
    "\n",
    "# ==========================================\n",
    "# STATE AND REWARD FUNCTIONS\n",
    "# ==========================================\n",
    "def get_state_eval(tls_id):\n",
    "    \"\"\"Get state for evaluation\"\"\"\n",
    "    lanes = traci.trafficlight.getControlledLanes(tls_id)\n",
    "    halting = [traci.lane.getLastStepHaltingNumber(lane) for lane in lanes]\n",
    "    \n",
    "    total_queue = sum(halting) / 50.0\n",
    "    max_queue = max(halting) / 20.0\n",
    "    phase = traci.trafficlight.getPhase(tls_id) / 4.0\n",
    "    \n",
    "    return np.array([total_queue, max_queue, phase], dtype=np.float32)\n",
    "\n",
    "def get_reward_eval(tls_id, waiting_time_prev):\n",
    "    \"\"\"Get reward for evaluation\"\"\"\n",
    "    lanes = traci.trafficlight.getControlledLanes(tls_id)\n",
    "    current_wait = sum([traci.lane.getWaitingTime(lane) for lane in lanes])\n",
    "    current_queue = sum([traci.lane.getLastStepHaltingNumber(lane) for lane in lanes])\n",
    "    \n",
    "    diff = waiting_time_prev - current_wait\n",
    "    reward = (diff * 0.2) - (current_queue * 0.05)\n",
    "    reward = max(min(reward, 5.0), -5.0)\n",
    "    \n",
    "    return reward, current_wait\n",
    "\n",
    "# ==========================================\n",
    "# EVALUATION MAIN FUNCTION\n",
    "# ==========================================\n",
    "def evaluate_rl_agent():\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"RL AGENT EVALUATION (Using final_model.pth)\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(f\"Model path: {FINAL_MODEL_PATH}\")\n",
    "    \n",
    "    # Load trained model\n",
    "    agent = DQNAgent(state_size=3, action_size=2)\n",
    "    if not agent.load(FINAL_MODEL_PATH):\n",
    "        print(\"ERROR: Model not found!\")\n",
    "        return False\n",
    "    \n",
    "    conn = None\n",
    "    sumo_bin = os.path.join(os.environ['SUMO_HOME'], 'bin', 'sumo')\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nStarting SUMO evaluation...\")\n",
    "        \n",
    "        # Build SUMO command\n",
    "        sumo_cmd = [\n",
    "            sumo_bin, \n",
    "            \"-c\", CONFIG_PATH,\n",
    "            \"--quit-on-end\",\n",
    "            \"--no-step-log\",\n",
    "            \"--waiting-time-memory\", \"1000\",\n",
    "            \"--tripinfo-output\", os.path.join(RESULTS_DIR, \"tripinfo_eval_ep999.xml\"),\n",
    "            \"--edgedata-output\", os.path.join(RESULTS_DIR, \"edge_data_eval_ep999.xml\")\n",
    "        ]\n",
    "        \n",
    "        # Start SUMO directly with traci\n",
    "        traci.start(sumo_cmd)\n",
    "        conn = traci.getConnection()\n",
    "        \n",
    "        print(\"SUMO started successfully\")\n",
    "        \n",
    "        # Initialize traffic lights\n",
    "        for tls in TLS_IDS:\n",
    "            traci.trafficlight.setProgram(tls, \"0\")\n",
    "        \n",
    "        tls_data = {t: {\"prev_wait\": 0, \"phase_time\": 0} for t in TLS_IDS}\n",
    "        total_reward = 0\n",
    "        \n",
    "        print(\"Running evaluation simulation (1 hour)...\")\n",
    "        \n",
    "        step_count = 0\n",
    "        while traci.simulation.getTime() < 3600:  # 1 hour\n",
    "            traci.simulationStep()\n",
    "            step_count += 1\n",
    "            \n",
    "            # Make decisions every 5 seconds\n",
    "            if step_count % 5 == 0:\n",
    "                for tls in TLS_IDS:\n",
    "                    # Get state and action (inference mode)\n",
    "                    state = get_state_eval(tls)\n",
    "                    action = agent.act_inference(state)\n",
    "                    \n",
    "                    # Apply action\n",
    "                    current_phase = traci.trafficlight.getPhase(tls)\n",
    "                    \n",
    "                    if current_phase == 1 or current_phase == 3:\n",
    "                        # Yellow phase logic\n",
    "                        tls_data[tls][\"phase_time\"] += 5\n",
    "                        if tls_data[tls][\"phase_time\"] >= YELLOW_DURATION:\n",
    "                            next_phase = 2 if current_phase == 1 else 0\n",
    "                            traci.trafficlight.setPhase(tls, next_phase)\n",
    "                            tls_data[tls][\"phase_time\"] = 0\n",
    "                    else:\n",
    "                        # Green phase logic\n",
    "                        target_phase = 0 if action == 0 else 2\n",
    "                        \n",
    "                        if current_phase != target_phase:\n",
    "                            if tls_data[tls][\"phase_time\"] >= MIN_GREEN:\n",
    "                                traci.trafficlight.setPhase(tls, current_phase + 1)\n",
    "                                tls_data[tls][\"phase_time\"] = 0\n",
    "                            else:\n",
    "                                tls_data[tls][\"phase_time\"] += 5\n",
    "                        else:\n",
    "                            tls_data[tls][\"phase_time\"] += 5\n",
    "                    \n",
    "                    # Calculate reward\n",
    "                    reward, new_wait = get_reward_eval(tls, tls_data[tls][\"prev_wait\"])\n",
    "                    tls_data[tls][\"prev_wait\"] = new_wait\n",
    "                    total_reward += reward\n",
    "        \n",
    "        print(f\"\\n✓ Evaluation completed successfully!\")\n",
    "        print(f\"  Total reward: {total_reward:.2f}\")\n",
    "        print(f\"  Simulation steps: {step_count}\")\n",
    "        print(f\"  Results saved to: {RESULTS_DIR}\")\n",
    "        \n",
    "        traci.close()\n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Evaluation failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "    \n",
    "    finally:\n",
    "        try:\n",
    "            if traci.isLoaded():\n",
    "                traci.close()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# Run evaluation\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Starting RL Agent Evaluation\")\n",
    "print(\"=\"*70)\n",
    "try:\n",
    "    success = evaluate_rl_agent()\n",
    "    if not success:\n",
    "        print(\"\\nPlease ensure final_model.pth exists before running evaluation.\")\n",
    "except Exception as e:\n",
    "    print(f\"Evaluation error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417bdf93",
   "metadata": {},
   "source": [
    "## KPI summary for the RL agent scenario\n",
    "\n",
    "This section reads the evaluation outputs stored in **Network with RL control/results** and reports key indicators plus comprehensive visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3ec2eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results directory: c:\\Users\\antoi\\OneDrive\\Documents\\Documents\\Devoirs\\Études sup\\ENTPE 3A\\Majeure transports\\Mobility Control and Management\\Project\\Livrable 2\\MOCOM_project_2\\Network with RL control\\results\n",
      "Model loaded from: c:\\Users\\antoi\\OneDrive\\Documents\\Documents\\Devoirs\\Études sup\\ENTPE 3A\\Majeure transports\\Mobility Control and Management\\Project\\Livrable 2\\MOCOM_project_2\\Network with RL control\\models\\final_model.pth\n",
      "Model exists: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "BASE_DIR = os.getcwd()\n",
    "SCENARIO_DIR = os.path.join(BASE_DIR, \"Network with RL control\")\n",
    "RESULTS_DIR = os.path.join(SCENARIO_DIR, \"results\")\n",
    "MODEL_DIR = os.path.join(SCENARIO_DIR, \"models\")\n",
    "FINAL_MODEL_PATH = os.path.join(MODEL_DIR, \"final_model.pth\")\n",
    "\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"Model loaded from: {FINAL_MODEL_PATH}\")\n",
    "print(f\"Model exists: {os.path.exists(FINAL_MODEL_PATH)}\")\n",
    "\n",
    "# Definition of the two directions\n",
    "DIR_A_EDGES = [\"E0E1\", \"E1E2\", \"E2E3\", \"E3E4\", \"E4E5\"]  # Forward (E0 -> E5)\n",
    "DIR_B_EDGES = [\"E5E4\", \"E4E3\", \"E3E2\", \"E2E1\", \"E1E0\"]  # Backward (E5 -> E0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b527fc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. VEHICLE CLASSIFICATION\n",
    "# ==========================================\n",
    "def classify_vehicle(veh_id):\n",
    "    \"\"\"Classify vehicles based on their ID prefix.\"\"\"\n",
    "    prefix = veh_id.split('.')[0] if '.' in veh_id else veh_id\n",
    "    \n",
    "    # --- Direction A (E0 -> E5) ---\n",
    "    if prefix in ['f_7']: \n",
    "        return \"Bus Dir A (E0->E5)\", \"Bus\", \"Dir A\"\n",
    "    \n",
    "    # --- Direction B (E5 -> E0) ---\n",
    "    elif prefix in ['f_6']: \n",
    "        return \"Bus Dir B (E5->E0)\", \"Bus\", \"Dir B\"\n",
    "        \n",
    "    # --- Other categories ---\n",
    "    elif prefix in ['f_4', 'f_5']:\n",
    "        return \"Other Bus\", \"Bus\", \"Other\"\n",
    "    elif prefix in ['f_0', 'f_1', 'f_2', 'f_3']:\n",
    "        return \"Transversal Traffic\", \"Traffic\", \"Transversal\"\n",
    "    else:\n",
    "        return \"Background\", \"Car\", \"Other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00043fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. DATA LOADING\n",
    "# ==========================================\n",
    "def load_data(data_folder):\n",
    "    \"\"\"Load trip and edge data from XML files (evaluation results).\"\"\"\n",
    "    trip_path = os.path.join(data_folder, \"tripinfo_eval_ep999.xml\")\n",
    "    edge_path = os.path.join(data_folder, \"edge_data_eval_ep999.xml\")\n",
    "\n",
    "    # --- 1. TRIPINFO (Individual vehicles) ---\n",
    "    trips = []\n",
    "    if os.path.exists(trip_path):\n",
    "        tree = ET.parse(trip_path)\n",
    "        for t in tree.getroot().findall('tripinfo'):\n",
    "            cat, vtype, route_group = classify_vehicle(t.get('id'))\n",
    "            \n",
    "            duration = float(t.get('duration'))\n",
    "            route_len = float(t.get('routeLength', 0))\n",
    "            \n",
    "            # Calculate average trip speed in km/h\n",
    "            speed_kmh = (route_len / duration) * 3.6 if duration > 0 else 0\n",
    "\n",
    "            trips.append({\n",
    "                'id': t.get('id'),\n",
    "                'category': cat,\n",
    "                'type': vtype,\n",
    "                'route_group': route_group,\n",
    "                'waitingTime': float(t.get('waitingTime')),\n",
    "                'duration': duration,\n",
    "                'routeLength': route_len,\n",
    "                'speed_kmh': speed_kmh\n",
    "            })\n",
    "    df_trips = pd.DataFrame(trips)\n",
    "\n",
    "    # --- 2. EDGE DATA (Roads) ---\n",
    "    edges_stats = []\n",
    "    if os.path.exists(edge_path):\n",
    "        tree = ET.parse(edge_path)\n",
    "        intervals = tree.getroot().findall('interval')\n",
    "        if intervals:\n",
    "            # Use the last interval\n",
    "            for e in intervals[-1].findall('edge'):\n",
    "                eid = e.get('id')\n",
    "                direction = \"None\"\n",
    "                order = 99\n",
    "                \n",
    "                if eid in DIR_A_EDGES:\n",
    "                    direction = \"Dir A (E0->E5)\"\n",
    "                    order = DIR_A_EDGES.index(eid)\n",
    "                elif eid in DIR_B_EDGES:\n",
    "                    direction = \"Dir B (E5->E0)\"\n",
    "                    order = DIR_B_EDGES.index(eid)\n",
    "                \n",
    "                if direction != \"None\":\n",
    "                    speed_ms = float(e.get('speed', 0))\n",
    "                    wait_sec = float(e.get('waitingTime', 0)) \n",
    "                    \n",
    "                    edges_stats.append({\n",
    "                        'edge_id': eid,\n",
    "                        'direction': direction,\n",
    "                        'order': order,\n",
    "                        'speed_kmh': speed_ms * 3.6,        # m/s -> km/h\n",
    "                        'waiting_hours': wait_sec / 3600.0, # seconds -> hours\n",
    "                        'density': float(e.get('density', 0))\n",
    "                    })\n",
    "    \n",
    "    df_edges = pd.DataFrame(edges_stats)\n",
    "    if not df_edges.empty:\n",
    "        df_edges = df_edges.sort_values(by=['direction', 'order'])\n",
    "\n",
    "    return df_trips, df_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70838a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4. VISUALIZATION\n",
    "# ==========================================\n",
    "def create_dashboard(df_trips, df_edges, output_dir):\n",
    "    \"\"\"Create comprehensive KPI dashboards.\"\"\"\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    \n",
    "    # Separate edge datasets by direction\n",
    "    df_edges_A = df_edges[df_edges['direction'] == \"Dir A (E0->E5)\"]\n",
    "    df_edges_B = df_edges[df_edges['direction'] == \"Dir B (E5->E0)\"]\n",
    "\n",
    "    # --- FIGURE 1: EDGE ANALYSIS (SPEED & WAIT) ---\n",
    "    fig1, axes = plt.subplots(3, 2, figsize=(16, 12), constrained_layout=True)\n",
    "    fig1.suptitle(\"Edge Analysis: Bidirectional Flow (km/h & Hours)\", fontsize=18, fontweight='bold')\n",
    "\n",
    "    # Column titles\n",
    "    axes[0,0].set_title(\"DIRECTION A (E0 -> E5)\", fontsize=14, color='green', fontweight='bold')\n",
    "    axes[0,1].set_title(\"DIRECTION B (E5 -> E0)\", fontsize=14, color='blue', fontweight='bold')\n",
    "\n",
    "    # ROW 1: SPEED (km/h)\n",
    "    if not df_edges_A.empty:\n",
    "        sns.lineplot(data=df_edges_A, x='edge_id', y='speed_kmh', marker='o', color='green', ax=axes[0,0], linewidth=3)\n",
    "        axes[0,0].set_ylabel(\"Speed (km/h)\")\n",
    "        for x, y in zip(range(len(df_edges_A)), df_edges_A['speed_kmh']):\n",
    "            axes[0,0].text(x, y+0.5, f\"{y:.1f}\", ha='center', color='green', fontweight='bold')\n",
    "            \n",
    "    if not df_edges_B.empty:\n",
    "        sns.lineplot(data=df_edges_B, x='edge_id', y='speed_kmh', marker='o', color='blue', ax=axes[0,1], linewidth=3)\n",
    "        axes[0,1].set_ylabel(\"\")\n",
    "        for x, y in zip(range(len(df_edges_B)), df_edges_B['speed_kmh']):\n",
    "            axes[0,1].text(x, y+0.5, f\"{y:.1f}\", ha='center', color='blue', fontweight='bold')\n",
    "\n",
    "    # ROW 2: TOTAL ACCUMULATED WAITING TIME (Hours)\n",
    "    if not df_edges_A.empty:\n",
    "        sns.barplot(data=df_edges_A, x='edge_id', y='waiting_hours', hue='edge_id', palette=\"Greens\", ax=axes[1,0], legend=False, edgecolor='black')\n",
    "        axes[1,0].set_ylabel(\"Total Accumulated Wait (Hours)\")\n",
    "        \n",
    "    if not df_edges_B.empty:\n",
    "        sns.barplot(data=df_edges_B, x='edge_id', y='waiting_hours', hue='edge_id', palette=\"Blues\", ax=axes[1,1], legend=False, edgecolor='black')\n",
    "        axes[1,1].set_ylabel(\"\")\n",
    "\n",
    "    # ROW 3: HEATMAPS (Hours)\n",
    "    if not df_edges_A.empty:\n",
    "        matrix_A = df_edges_A[['edge_id', 'waiting_hours']].set_index('edge_id').T\n",
    "        sns.heatmap(matrix_A, annot=True, fmt=\".2f\", cmap=\"Reds\", ax=axes[2,0], cbar=False)\n",
    "        axes[2,0].set_xlabel(\"Edge Sequence\")\n",
    "        \n",
    "    if not df_edges_B.empty:\n",
    "        matrix_B = df_edges_B[['edge_id', 'waiting_hours']].set_index('edge_id').T\n",
    "        sns.heatmap(matrix_B, annot=True, fmt=\".2f\", cmap=\"Reds\", ax=axes[2,1], cbar_kws={'label': 'Hours'})\n",
    "        axes[2,1].set_xlabel(\"Edge Sequence\")\n",
    "\n",
    "    plot_path_1 = os.path.join(output_dir, \"edge_analysis_bidirectional.png\")\n",
    "    plt.savefig(plot_path_1, dpi=300)\n",
    "    print(f\"Saved: {plot_path_1}\")\n",
    "\n",
    "    # --- FIGURE 2: BUS vs NETWORK COMPARISON ---\n",
    "    fig2, axes = plt.subplots(2, 2, figsize=(14, 10), constrained_layout=True)\n",
    "    fig2.suptitle(\"Strategic KPI: Bus Performance Analysis\", fontsize=16, fontweight='bold')\n",
    "\n",
    "    # 1. Waiting Time Comparison\n",
    "    target_buses = df_trips[df_trips['type'] == 'Bus']\n",
    "    if not target_buses.empty:\n",
    "        sns.barplot(data=target_buses, x='category', y='waitingTime', hue='category', palette=\"viridis\", ax=axes[0,0], legend=False)\n",
    "        axes[0,0].set_title(\"Avg Bus Trip Waiting Time (Seconds)\", fontweight='bold')\n",
    "        axes[0,0].set_ylabel(\"Seconds per Trip\")\n",
    "        for c in axes[0,0].containers:\n",
    "            axes[0,0].bar_label(c, fmt='%.1f', padding=3)\n",
    "\n",
    "    # 2. Speed Comparison\n",
    "    comp_groups = [\"Bus Dir A (E0->E5)\", \"Bus Dir B (E5->E0)\", \"Transversal Traffic\"]\n",
    "    speed_df = df_trips[df_trips['category'].isin(comp_groups)]\n",
    "    \n",
    "    if not speed_df.empty:\n",
    "        sns.boxplot(data=speed_df, x='category', y='speed_kmh', hue='category', palette=\"Set2\", ax=axes[0,1], legend=False)\n",
    "        axes[0,1].set_title(\"Speed Comparison: Bus vs Cross Traffic\", fontweight='bold')\n",
    "        axes[0,1].set_ylabel(\"Mean Trip Speed (km/h)\")\n",
    "\n",
    "    # 3. Duration Breakdown - Bottom Left\n",
    "    dur_df = df_trips.groupby('category')['duration'].mean().reset_index()\n",
    "    dur_df = dur_df[dur_df['category'].isin(comp_groups + [\"Other Bus\"])]\n",
    "    \n",
    "    if not dur_df.empty:\n",
    "        sns.barplot(data=dur_df, x='category', y='duration', hue='category', palette=\"coolwarm\", ax=axes[1,0], legend=False)\n",
    "        axes[1,0].set_title(\"Average Trip Duration\", fontweight='bold')\n",
    "        axes[1,0].set_ylabel(\"Duration (seconds)\")\n",
    "        for c in axes[1,0].containers:\n",
    "            axes[1,0].bar_label(c, fmt='%.1f', padding=3)\n",
    "\n",
    "    # 4. Text Summary Box - Bottom Right\n",
    "    axes[1,1].axis('off')\n",
    "    \n",
    "    text_str = \"KPI SUMMARY REPORT\\n\" + \"=\"*35 + \"\\n\"\n",
    "    \n",
    "    # Global network summary\n",
    "    text_str += \"GLOBAL NETWORK SUMMARY:\\n\"\n",
    "    text_str += f\"  - Total Vehicles: {len(df_trips)}\\n\"\n",
    "    text_str += f\"  - Avg Waiting:    {df_trips['waitingTime'].mean():.2f} s\\n\"\n",
    "    text_str += f\"  - Avg Speed:      {df_trips['speed_kmh'].mean():.2f} km/h\\n\"\n",
    "    text_str += \"-\"*35 + \"\\n\"\n",
    "\n",
    "    # Category breakdown\n",
    "    for cat in comp_groups:\n",
    "        subset = df_trips[df_trips['category'] == cat]\n",
    "        if not subset.empty:\n",
    "            avg_wait = subset['waitingTime'].mean()\n",
    "            avg_speed = subset['speed_kmh'].mean()\n",
    "            avg_duration = subset['duration'].mean()\n",
    "            count = len(subset)\n",
    "            text_str += f\"{cat}:\\n\"\n",
    "            text_str += f\"  - Vehicles:  {count}\\n\"\n",
    "            text_str += f\"  - Avg Speed: {avg_speed:.2f} km/h\\n\"\n",
    "            text_str += f\"  - Avg Wait:  {avg_wait:.2f} s\\n\"\n",
    "            text_str += f\"  - Avg Dur:   {avg_duration:.2f} s\\n\\n\"\n",
    "    \n",
    "    axes[1,1].text(0.05, 0.95, text_str, fontsize=10, fontfamily='monospace', verticalalignment='top', \n",
    "                   bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.5))\n",
    "\n",
    "    plot_path_2 = os.path.join(output_dir, \"kpi_analysis_performance.png\")\n",
    "    plt.savefig(plot_path_2, dpi=300)\n",
    "    print(f\"Saved: {plot_path_2}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768b3d16",
   "metadata": {},
   "source": [
    "## KPI Analysis Results\n",
    "\n",
    "This section loads and visualizes the performance data from the RL Agent evaluation (episode 999)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1f39645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from: c:\\Users\\antoi\\OneDrive\\Documents\\Documents\\Devoirs\\Études sup\\ENTPE 3A\\Majeure transports\\Mobility Control and Management\\Project\\Livrable 2\\MOCOM_project_2\\Network with RL control\\results\n",
      "Saving plots to: c:\\Users\\antoi\\OneDrive\\Documents\\Documents\\Devoirs\\Études sup\\ENTPE 3A\\Majeure transports\\Mobility Control and Management\\Project\\Livrable 2\\MOCOM_project_2\\Network with RL control\\results\n",
      "ERROR: no element found: line 5923, column 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\antoi\\AppData\\Local\\Temp\\ipykernel_9600\\3176129088.py\", line 8, in <module>\n",
      "    trips, edges = load_data(RESULTS_DIR)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\antoi\\AppData\\Local\\Temp\\ipykernel_9600\\3096874521.py\", line 12, in load_data\n",
      "    tree = ET.parse(trip_path)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\antoi\\miniconda3\\envs\\MCM\\Lib\\xml\\etree\\ElementTree.py\", line 1219, in parse\n",
      "    tree.parse(source, parser)\n",
      "  File \"c:\\Users\\antoi\\miniconda3\\envs\\MCM\\Lib\\xml\\etree\\ElementTree.py\", line 581, in parse\n",
      "    self._root = parser._parse_whole(source)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "xml.etree.ElementTree.ParseError: no element found: line 5923, column 0\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 5. EXECUTION\n",
    "# ==========================================\n",
    "print(f\"Reading data from: {RESULTS_DIR}\")\n",
    "print(f\"Saving plots to: {RESULTS_DIR}\")\n",
    "\n",
    "try:\n",
    "    trips, edges = load_data(RESULTS_DIR)\n",
    "    \n",
    "    if trips.empty:\n",
    "        print(\"\\nWARNING: No trip data found.\")\n",
    "        print(\"Please ensure the evaluation simulation has been run.\")\n",
    "        print(f\"Expected file: {os.path.join(RESULTS_DIR, 'tripinfo_eval_ep999.xml')}\")\n",
    "    else:\n",
    "        print(f\"\\nLoaded {len(trips)} trips and {len(edges)} edge records\")\n",
    "        create_dashboard(trips, edges, RESULTS_DIR)\n",
    "        print(\"\\nAnalysis completed successfully!\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"ERROR: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MCM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
