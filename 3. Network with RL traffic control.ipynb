{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0149d6b9",
   "metadata": {},
   "source": [
    "# RL Agent traffic light control (SUMO + TraCI + PyTorch)\n",
    "\n",
    "This notebook trains and evaluates an **RL agent** for traffic control using a simple neural network.\n",
    "\n",
    "## Algorithm Overview\n",
    "\n",
    "The RL Agent learning approach:\n",
    "1. **Brain (Neural Network)**: A small PyTorch network that takes queue state (4 inputs) and outputs action probabilities (2 actions)\n",
    "2. **State**: Normalized incoming vehicle counts from 4 directions (N, S, E, W)\n",
    "3. **Action**: Binary choice - phase 0 (N-S green) or phase 2 (E-W green)\n",
    "4. **Reward**: Negative of average waiting time (lower waiting = higher reward)\n",
    "5. **Safety**: Yellow phase enforced between transitions\n",
    "\n",
    "## Workflow\n",
    "1. **Training Phase**: Learn policy from trial-and-error over multiple episodes\n",
    "2. **Evaluation Phase**: Test trained agent on fresh scenario\n",
    "3. **KPI Analysis**: Report metrics compared to baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9457ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# ==========================================\n",
    "# DEVICE SELECTION (CPU/GPU)\n",
    "# ==========================================\n",
    "# Check if CUDA (NVIDIA GPU) is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available!\")\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    \n",
    "    # Ask user for device selection\n",
    "    use_gpu = input(\"\\nDo you want to use GPU for training? (y/n): \").strip().lower()\n",
    "    if use_gpu == 'y':\n",
    "        device = torch.device('cuda')\n",
    "        print(f\"\\n✓ Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        print(\"\\n✓ Using CPU\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\\n✓ Using CPU\")\n",
    "\n",
    "print(f\"\\nSelected device: {device}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f502f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SUMO binary: C:\\Program Files (x86)\\Eclipse\\Sumo\\bin\\sumo-gui.exe\n",
      "\n",
      "======================================================================\n",
      "RL AGENT TRAINING STARTED\n",
      "======================================================================\n",
      "\n",
      "--- Episode 1/5 ---\n",
      "Episode 1 completed: avg_reward = -14.3365\n",
      "\n",
      "--- Episode 2/5 ---\n",
      "Episode 2 completed: avg_reward = -15.1301\n",
      "\n",
      "--- Episode 3/5 ---\n",
      "Episode 3 completed: avg_reward = -13.7541\n",
      "\n",
      "--- Episode 4/5 ---\n",
      "Episode 4 completed: avg_reward = -14.7695\n",
      "\n",
      "--- Episode 5/5 ---\n",
      "Episode 5 completed: avg_reward = -13.5166\n",
      "Model saved to c:\\Users\\antoi\\OneDrive\\Documents\\Documents\\Devoirs\\Études sup\\ENTPE 3A\\Majeure transports\\Mobility Control and Management\\Project\\Livrable 2\\MOCOM_project_2\\Network with RL control\\models\\traffic_agent.pth\n",
      "\n",
      "======================================================================\n",
      "TRAINING COMPLETED\n",
      "======================================================================\n",
      "\n",
      "Training log summary:\n",
      "  Episode 1: avg_reward = -14.3365\n",
      "  Episode 2: avg_reward = -15.1301\n",
      "  Episode 3: avg_reward = -13.7541\n",
      "  Episode 4: avg_reward = -14.7695\n",
      "  Episode 5: avg_reward = -13.5166\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import subprocess\n",
    "import socket\n",
    "\n",
    "# ==========================================\n",
    "# 1. NEURAL NETWORK BRAIN\n",
    "# ==========================================\n",
    "class TrafficBrain(nn.Module):\n",
    "    \"\"\"Simple 2-layer neural network for traffic light control.\"\"\"\n",
    "    def __init__(self, input_size=4, hidden_size=32, output_size=2, device='cpu'):\n",
    "        super(TrafficBrain, self).__init__()\n",
    "        self.device = device\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return torch.softmax(x, dim=-1)\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "        torch.save(self.state_dict(), filepath)\n",
    "        print(f\"Model saved to {filepath}\")\n",
    "    \n",
    "    def load(self, filepath, device='cpu'):\n",
    "        if os.path.exists(filepath):\n",
    "            self.load_state_dict(torch.load(filepath, map_location=device))\n",
    "            print(f\"Model loaded from {filepath}\")\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 2. CONFIGURATION (RL TRAINING)\n",
    "# ==========================================\n",
    "BASE_DIR = os.getcwd()\n",
    "SCENARIO_DIR = os.path.join(BASE_DIR, \"Network with RL control\")\n",
    "RESULTS_DIR = os.path.join(SCENARIO_DIR, \"results\")\n",
    "CONFIG_PATH = os.path.join(SCENARIO_DIR, \"ff_heterogeneous.sumocfg\")\n",
    "MODEL_DIR = os.path.join(SCENARIO_DIR, \"models\")\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, \"traffic_agent.pth\")\n",
    "\n",
    "if not os.path.exists(SCENARIO_DIR):\n",
    "    # Create directory structure if it doesn't exist\n",
    "    print(f\"Creating scenario folder: {SCENARIO_DIR}\")\n",
    "    os.makedirs(SCENARIO_DIR, exist_ok=True)\n",
    "    # Copy network files from original network\n",
    "    import shutil\n",
    "    original_dir = os.path.join(BASE_DIR, \"Original network\")\n",
    "    for file in [\"ff_heterogeneous.sumocfg\", \"ff.net.xml\", \"ff_heterogeneous.rou.xml\"]:\n",
    "        src = os.path.join(original_dir, file)\n",
    "        dst = os.path.join(SCENARIO_DIR, file)\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy(src, dst)\n",
    "            print(f\"Copied {file}\")\n",
    "\n",
    "if not os.path.exists(RESULTS_DIR):\n",
    "    print(f\"Creating results folder: {RESULTS_DIR}\")\n",
    "    os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(CONFIG_PATH):\n",
    "    sys.exit(f\"Config file not found: {CONFIG_PATH}\")\n",
    "\n",
    "# SUMO tools\n",
    "if 'SUMO_HOME' in os.environ:\n",
    "    tools = os.path.join(os.environ['SUMO_HOME'], 'tools')\n",
    "    sys.path.append(tools)\n",
    "else:\n",
    "    sys.exit(\"Please declare environment variable 'SUMO_HOME'\")\n",
    "\n",
    "import traci\n",
    "\n",
    "# SUMO binary resolution (headless mode for faster training)\n",
    "sumoBinary = \"sumo\"\n",
    "if 'SUMO_HOME' in os.environ:\n",
    "    candidate = os.path.join(os.environ['SUMO_HOME'], 'bin', 'sumo.exe')\n",
    "    if os.path.exists(candidate):\n",
    "        sumoBinary = candidate\n",
    "    else:\n",
    "        candidate = os.path.join(os.environ['SUMO_HOME'], 'bin', 'sumo')\n",
    "        if os.path.exists(candidate):\n",
    "            sumoBinary = candidate\n",
    "\n",
    "if not os.path.exists(sumoBinary):\n",
    "    try:\n",
    "# Logs - save in results folder\n",
    "sumo_log = os.path.join(RESULTS_DIR, \"sumo_log.txt\")\n",
    "sumo_err = os.path.join(RESULTS_DIR, \"sumo_err.txt\")\n",
    "traci_stdout = os.path.join(RESULTS_DIR, \"traci_stdout.txt\")\n",
    "\n",
    "# Output files - save in results folder\n",
    "EDGE_DATA_PATH = os.path.join(RESULTS_DIR, \"edge_data.xml\")\n",
    "# Logs\n",
    "sumo_log = os.path.join(SCENARIO_DIR, \"sumo_log.txt\")\n",
    "sumo_err = os.path.join(SCENARIO_DIR, \"sumo_err.txt\")\n",
    "traci_stdout = os.path.join(SCENARIO_DIR, \"traci_stdout.txt\")\n",
    "\n",
    "# Output files\n",
    "EDGE_DATA_PATH = os.path.join(SCENARIO_DIR, \"edge_data.xml\")\n",
    "\n",
    "\n",
    "def get_free_port():\n",
    "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "        s.bind((\"localhost\", 0))\n",
    "        return s.getsockname()[1]\n",
    "\n",
    "\n",
    "# Training parameters\n",
    "NUM_EPISODES = 1000  # Number of training episodes\n",
    "EPISODE_DURATION = 3600  # seconds\n",
    "YELLOW_DURATION = 3  # seconds\n",
    "MIN_GREEN = 5  # seconds\n",
    "    output_file = os.path.join(RESULTS_DIR, f\"tripinfo_{mode}_ep{episode_num}.xml\")\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "\n",
    "TLS_IDS = [\"E1\", \"E2\", \"E3\", \"E4\"]\n",
    "\n",
    "\n",
    "def start_sumo(episode_num, mode=\"train\"):\n",
    "    output_file = os.path.join(SCENARIO_DIR, f\"tripinfo_{mode}_ep{episode_num}.xml\")\n",
    "    log_handle = open(traci_stdout, \"a\", encoding=\"utf-8\")\n",
    "        \"--emission-output\", os.path.join(RESULTS_DIR, f\"emissions_{mode}_ep{episode_num}.xml\"),\n",
    "        \"--edgedata-output\", os.path.join(RESULTS_DIR, f\"edge_data_{mode}_ep{episode_num}.xml\"),\n",
    "        sumoBinary,\n",
    "        \"-c\", CONFIG_PATH,\n",
    "        \"--quit-on-end\",\n",
    "        \"--no-step-log\",  # Disable step logging for faster execution\n",
    "        \"--tripinfo-output\", output_file,\n",
    "        \"--emission-output\", os.path.join(SCENARIO_DIR, f\"emissions_{mode}_ep{episode_num}.xml\"),\n",
    "        \"--edgedata-output\", os.path.join(SCENARIO_DIR, f\"edge_data_{mode}_ep{episode_num}.xml\"),\n",
    "        \"--log\", sumo_log,\n",
    "        \"--error-log\", sumo_err,\n",
    "        \"--remote-port\", str(get_free_port())\n",
    "    ]\n",
    "    \n",
    "    proc = subprocess.Popen(\n",
    "        sumoCmd,\n",
    "        cwd=SCENARIO_DIR,\n",
    "        stdout=log_handle,\n",
    "        stderr=log_handle\n",
    "    )\n",
    "    return proc, log_handle, sumoCmd[-1]\n",
    "\n",
    "\n",
    "def connect_traci(port_str, timeout_s=10):\n",
    "    port = int(port_str)\n",
    "    deadline = time.time() + timeout_s\n",
    "    last_error = None\n",
    "    while time.time() < deadline:\n",
    "        try:\n",
    "            return traci.connect(port=port, host=\"localhost\", numRetries=0, waitBetweenRetries=0)\n",
    "        except Exception as e:\n",
    "            last_error = e\n",
    "            time.sleep(0.2)\n",
    "    raise RuntimeError(f\"Could not connect. Last error: {last_error}\")\n",
    "\n",
    "\n",
    "def set_safe_phase(conn, tls_id, target_phase):\n",
    "    \"\"\"\n",
    "    Safely transition to target phase with yellow light.\n",
    "    Returns: timesteps spent in yellow\n",
    "    \"\"\"\n",
    "    current_phase = conn.trafficlight.getPhase(tls_id)\n",
    "    if current_phase == target_phase:\n",
    "        return 0\n",
    "    \n",
    "    # Go to yellow phase\n",
    "    conn.trafficlight.setPhase(tls_id, current_phase + 1)\n",
    "    for _ in range(YELLOW_DURATION):\n",
    "        conn.simulationStep()\n",
    "    \n",
    "    # Go to target phase\n",
    "    conn.trafficlight.setPhase(tls_id, target_phase)\n",
    "    return YELLOW_DURATION\n",
    "\n",
    "\n",
    "def get_state(conn):\n",
    "    \"\"\"\n",
    "    Get current state: queue lengths on incoming lanes normalized by 50.\n",
    "    Returns: torch.FloatTensor of shape (1,) on the selected device\n",
    "    \"\"\"\n",
    "    # Count vehicles in queue (halting) on each approach\n",
    "    lanes = conn.trafficlight.getControlledLanes(\"E1\")\n",
    "    queue_count = sum(conn.lane.getLastStepHaltingNumber(lane) for lane in lanes)\n",
    "    \n",
    "    # Simple state: normalize queue count and move to device\n",
    "    state = torch.FloatTensor([queue_count / 50.0]).to(device)\n",
    "    return state\n",
    "\n",
    "\n",
    "def get_reward(conn):\n",
    "    \"\"\"\n",
    "    Calculate reward based on queue length.\n",
    "    Reward = -queue_count (we want to minimize queues).\n",
    "    Lower queue = higher reward.\n",
    "    \"\"\"\n",
    "    lanes = conn.trafficlight.getControlledLanes(\"E1\")\n",
    "    queue_count = sum(conn.lane.getLastStepHaltingNumber(lane) for lane in lanes)\n",
    "    \n",
    "    # Reward is negative of queue count: fewer vehicles waiting = higher reward\n",
    "    reward = -float(queue_count)\n",
    "    return reward\n",
    "\n",
    "\n",
    "def train_rl_agent():\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"RL AGENT TRAINING STARTED (HEADLESS MODE)\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Training {NUM_EPISODES} episodes without GUI visualization\")\n",
    "    print(\"This will be significantly faster than GUI mode.\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Initialize brain and optimizer\n",
    "    brain = TrafficBrain(input_size=1, hidden_size=16, output_size=2, device=device).to(device)\n",
    "    optimizer = optim.Adam(brain.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    training_log = []\n",
    "    \n",
    "    for episode in range(NUM_EPISODES):\n",
    "        print(f\"\\n--- Episode {episode + 1}/{NUM_EPISODES} ---\")\n",
    "        \n",
    "        proc = None\n",
    "        log_handle = None\n",
    "        conn = None\n",
    "        port_str = None\n",
    "        \n",
    "        try:\n",
    "            proc, log_handle, port_str = start_sumo(episode, mode=\"train\")\n",
    "            time.sleep(1)  # Reduced wait time for headless mode\n",
    "            conn = connect_traci(port_str, timeout_s=10)\n",
    "            \n",
    "            # Initialize traffic light\n",
    "            for tls in TLS_IDS:\n",
    "                conn.trafficlight.setProgram(tls, \"0\")\n",
    "            \n",
    "            episode_reward = 0\n",
    "            episode_steps = 0\n",
    "            phase_times = {tls: 0 for tls in TLS_IDS}\n",
    "            action_history = []\n",
    "            \n",
    "            brain.train()\n",
    "            \n",
    "            while conn.simulation.getTime() <= EPISODE_DURATION:\n",
    "                conn.simulationStep()\n",
    "                sim_time = conn.simulation.getTime()\n",
    "                \n",
    "                # Decision every 10 steps (not every step to reduce overhead)\n",
    "                if int(sim_time) % 10 == 0:\n",
    "                    state = get_state(conn)\n",
    "                    reward = get_reward(conn)\n",
    "                    episode_reward += reward\n",
    "                    \n",
    "                    # Forward pass through brain\n",
    "                    with torch.no_grad():\n",
    "                        probs = brain(state)\n",
    "                    \n",
    "                    # Choose action (0=N-S, 1=E-W)\n",
    "                    action = torch.argmax(probs).item()\n",
    "                    target_phase = 0 if action == 0 else 2\n",
    "                    \n",
    "                    # Check if enough time in current phase\n",
    "                    if phase_times[\"E1\"] >= MIN_GREEN:\n",
    "                        # Apply action\n",
    "                        set_safe_phase(conn, \"E1\", target_phase)\n",
    "                        phase_times[\"E1\"] = 0\n",
    "                    else:\n",
    "                        phase_times[\"E1\"] += 1\n",
    "                    \n",
    "                    action_history.append(action)\n",
    "                    \n",
    "                    # Training step: policy gradient\n",
    "                    probs = brain(state)  # Recompute for gradient\n",
    "                    action_prob = probs[action]\n",
    "                    loss = -torch.log(action_prob + 1e-8) * reward\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                episode_steps += 1\n",
    "            \n",
    "            avg_episode_reward = episode_reward / episode_steps if episode_steps > 0 else 0\n",
    "            training_log.append({\n",
    "                'episode': episode + 1,\n",
    "                'avg_reward': avg_episode_reward,\n",
    "                'total_reward': episode_reward,\n",
    "                'steps': episode_steps\n",
    "            })\n",
    "            \n",
    "            print(f\"Episode {episode + 1} completed: avg_reward = {avg_episode_reward:.4f}\")\n",
    "            \n",
    "            # Save model periodically\n",
    "            if (episode + 1) % 50 == 0:\n",
    "                checkpoint_path = os.path.join(MODEL_DIR, f\"traffic_agent_ep{episode + 1}.pth\")\n",
    "                brain.save(checkpoint_path)\n",
    "            \n",
    "            if conn is not None:\n",
    "                conn.close()\n",
    "            if proc is not None:\n",
    "                proc.wait(timeout=5)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Episode {episode + 1} failed: {e}\")\n",
    "        \n",
    "        finally:\n",
    "            if conn is not None:\n",
    "                try:\n",
    "                    conn.close()\n",
    "                except:\n",
    "                    pass\n",
    "            if proc is not None and proc.poll() is None:\n",
    "                proc.terminate()\n",
    "            if log_handle:\n",
    "                log_handle.close()\n",
    "    \n",
    "    # Save trained model\n",
    "    brain.save(MODEL_PATH)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TRAINING COMPLETED\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return brain, training_log\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 3. EXECUTION\n",
    "# ==========================================\n",
    "\n",
    "try:    print(f\"Training failed: {e}\")\n",
    "\n",
    "    brain, training_log = train_rl_agent()except Exception as e:\n",
    "\n",
    "    print(f\"\\nTraining log summary:\")        print(f\"  Episode {log_entry['episode']}: avg_reward = {log_entry['avg_reward']:.4f}\")\n",
    "    for log_entry in training_log:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27118339",
   "metadata": {},
   "source": [
    "## Evaluation Phase\n",
    "\n",
    "Test the trained agent on a fresh scenario and compare against baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff5f3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "RL AGENT EVALUATION\n",
      "======================================================================\n",
      "Model loaded from c:\\Users\\antoi\\OneDrive\\Documents\\Documents\\Devoirs\\Études sup\\ENTPE 3A\\Majeure transports\\Mobility Control and Management\\Project\\Livrable 2\\MOCOM_project_2\\Network with RL control\\models\\traffic_agent.pth\n",
      "\n",
      "Running evaluation simulation...\n",
      "Evaluation completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# EVALUATION: Run trained agent\n",
    "# ==========================================\n",
    "\n",
    "def evaluate_rl_agent():\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"RL AGENT EVALUATION\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load trained model\n",
    "    brain = TrafficBrain(input_size=1, hidden_size=16, output_size=2, device=device).to(device)\n",
    "    if not brain.load(MODEL_PATH, device=device):\n",
    "        print(\"ERROR: Trained model not found! Run training cell first.\")\n",
    "        return\n",
    "    \n",
    "    brain.eval()\n",
    "    \n",
    "    proc = None\n",
    "    log_handle = None\n",
    "    conn = None\n",
    "    \n",
    "    try:\n",
    "        proc, log_handle, port_str = start_sumo(episode_num=999, mode=\"eval\")\n",
    "        time.sleep(2)\n",
    "        conn = connect_traci(port_str, timeout_s=10)\n",
    "        \n",
    "        # Initialize traffic light\n",
    "        for tls in TLS_IDS:\n",
    "            conn.trafficlight.setProgram(tls, \"0\")\n",
    "        \n",
    "        phase_times = {tls: 0 for tls in TLS_IDS}\n",
    "        \n",
    "        print(\"\\nRunning evaluation simulation...\")\n",
    "        \n",
    "        while conn.simulation.getTime() <= EPISODE_DURATION:\n",
    "            conn.simulationStep()\n",
    "            sim_time = conn.simulation.getTime()\n",
    "            \n",
    "            # Decision every 10 steps\n",
    "            if int(sim_time) % 10 == 0:\n",
    "                state = get_state(conn)\n",
    "                \n",
    "                # Forward pass (no gradient needed)\n",
    "                with torch.no_grad():\n",
    "                    probs = brain(state)\n",
    "                    action = torch.argmax(probs).item()\n",
    "                \n",
    "                target_phase = 0 if action == 0 else 2\n",
    "                \n",
    "                if phase_times[\"E1\"] >= MIN_GREEN:\n",
    "                    set_safe_phase(conn, \"E1\", target_phase)\n",
    "                    phase_times[\"E1\"] = 0\n",
    "                else:\n",
    "                    phase_times[\"E1\"] += 1\n",
    "        \n",
    "        if conn is not None:\n",
    "            conn.close()\n",
    "        if proc is not None:\n",
    "            proc.wait(timeout=5)\n",
    "        \n",
    "        print(\"Evaluation completed successfully!\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Evaluation failed: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            try:\n",
    "                conn.close()\n",
    "            except:\n",
    "                pass\n",
    "        if proc is not None and proc.poll() is None:\n",
    "            proc.terminate()\n",
    "        if log_handle:\n",
    "            log_handle.close()\n",
    "\n",
    "\n",
    "try:\n",
    "    evaluate_rl_agent()\n",
    "except Exception as e:\n",
    "    print(f\"Evaluation error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417bdf93",
   "metadata": {},
   "source": [
    "## KPI summary for the RL agent scenario\n",
    "\n",
    "This section reads the evaluation outputs and reports key indicators plus visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ec2eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ==========================================\n",
    "# 1. HELPERS\n",
    "# ==========================================\n",
    "BASE_DIR = os.getcwd()\n",
    "SCENARIO_DIR = os.path.join(BASE_DIR, \"Network with RL control\")\n",
    "RESULTS_DIR = os.path.join(SCENARIO_DIR, \"results\")\n",
    "EDGE_DATA_PATH = os.path.join(RESULTS_DIR, \"edge_data_eval_ep999.xml\")\n",
    "\n",
    "\n",
    "def parse_tripinfo(results_folder, filename_pattern=\"tripinfo_eval_ep999.xml\"):\n",
    "    path = os.path.join(results_folder, filename_pattern)\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Warning: {path} not found\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    tree = ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "    trips = []\n",
    "    for trip in root.findall('tripinfo'):\n",
    "        trips.append({\n",
    "            'id': trip.get('id'),\n",
    "            'duration': float(trip.get('duration')),\n",
    "            'waitingTime': float(trip.get('waitingTime')),\n",
    "            'timeLoss': float(trip.get('timeLoss'))\n",
    "        })\n",
    "    return pd.DataFrame(trips)\n",
    "\n",
    "def parse_emissions(results_folder, filename_pattern=\"emissions_eval_ep999.xml\"):\n",
    "    path = os.path.join(results_folder, filename_pattern)\n",
    "    path = os.path.join(folder, filename_pattern)\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    \n",
    "    tree = ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "    total = {'CO2': 0, 'NOx': 0, 'fuel': 0}\n",
    "    for timestep in root.findall('timestep'):\n",
    "        for veh in timestep.findall('vehicle'):\n",
    "            total['CO2'] += float(veh.get('CO2', 0))\n",
    "            total['NOx'] += float(veh.get('NOx', 0))\n",
    "            total['fuel'] += float(veh.get('fuel', 0))\n",
    "    return total\n",
    "\n",
    "\n",
    "def parse_edge_data(path):\n",
    "    if not os.path.exists(path):\n",
    "        return {}\n",
    "    \n",
    "    tree = ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "    intervals = root.findall('interval')\n",
    "    if not intervals:\n",
    "        return {}\n",
    "    \n",
    "    last_interval = intervals[-1]\n",
    "    edge_stats = {}\n",
    "    for edge in last_interval.findall('edge'):\n",
    "        e_id = edge.get('id')\n",
    "        edge_stats[e_id] = {\n",
    "            'speed': float(edge.get('speed')),\n",
    "            'waiting': float(edge.get('waitingTime')),\n",
    "            'density': float(edge.get('density'))\n",
    "        }\n",
    "    return edge_stats\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 2. KPI SUMMARY\n",
    "print(f\"Reading outputs from: {RESULTS_DIR}\")\n",
    "print(f\"Reading outputs from: {SCENARIO_DIR}\")\n",
    "df = parse_tripinfo(RESULTS_DIR)\n",
    "emissions = parse_emissions(RESULTS_DIR)\n",
    "emissions = parse_emissions(SCENARIO_DIR)\n",
    "edge_stats = parse_edge_data(EDGE_DATA_PATH)\n",
    "\n",
    "if len(df) == 0:\n",
    "    print(\"No trip data available. Make sure evaluation simulation completed.\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"KPI SUMMARY (RL AGENT)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Avg. Travel Time: {df['duration'].mean():.2f} s\")\n",
    "    print(f\"Avg. Waiting Time: {df['waitingTime'].mean():.2f} s\")\n",
    "    print(f\"Total Throughput: {len(df)} vehicles\")\n",
    "\n",
    "    if emissions:\n",
    "        print(\"-\" * 70)\n",
    "        print(f\"Total CO2 Emissions: {emissions['CO2'] / 1000000:.2f} kg\")\n",
    "        print(f\"Total NOx: {emissions['NOx']:.2f} g\")\n",
    "        print(f\"Total Fuel: {emissions['fuel']:.2f} mg\")\n",
    "\n",
    "    if edge_stats:\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"EDGE SUMMARY (E0->E5 and E5->E0)\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "        edges_forward = [\"E0E1\", \"E1E2\", \"E2E3\", \"E3E4\", \"E4E5\"]\n",
    "        edges_backward = [\"E5E4\", \"E4E3\", \"E3E2\", \"E2E1\", \"E1E0\"]\n",
    "\n",
    "        for direction, edges in [(\"E0->E5\", edges_forward), (\"E5->E0\", edges_backward)]:\n",
    "            print(f\"\\n{direction}\")\n",
    "            for e in edges:\n",
    "                stats = edge_stats.get(e, {'speed': 0, 'waiting': 0, 'density': 0})\n",
    "                speed_kmh = stats['speed'] * 3.6  # Convert m/s to km/h\n",
    "                print(f\"{e:<6} | speed: {speed_kmh:.2f} km/h | waiting: {stats['waiting']:.2f} s | density: {stats['density']:.2f} veh/km\")\n",
    "\n",
    "    # ==========================================\n",
    "    # 3. VISUALIZATION\n",
    "    # ==========================================\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.hist(df['waitingTime'], bins=30, alpha=0.7, color='green')\n",
    "    plt.title('Waiting Time Distribution (RL Agent)')\n",
    "    plt.xlabel('Seconds')\n",
    "    plt.ylabel('Number of Vehicles')\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plot_path = os.path.join(RESULTS_DIR, \"rl_waiting_time.png\")\n",
    "    plot_path = os.path.join(SCENARIO_DIR, \"rl_waiting_time.png\")\n",
    "    plt.savefig(plot_path, dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"\\nSaved plot to: {plot_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MCM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
