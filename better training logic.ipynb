{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b505d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TRAINING CONFIGURATION ---\n",
      "Device: cuda\n",
      "Target Time: ~1 Hour\n",
      "Episodes: 500\n",
      "Steps per Episode: 900\n",
      "------------------------------\n",
      "\n",
      "Starting Optimized Training...\n",
      "Ep 1/500 | Reward: -594.85 | Eps: 0.99\n",
      "Ep 2/500 | Reward: -578.00 | Eps: 0.98\n",
      "Ep 3/500 | Reward: -607.90 | Eps: 0.97\n",
      "Ep 4/500 | Reward: -497.30 | Eps: 0.96\n",
      "Ep 5/500 | Reward: -504.60 | Eps: 0.95\n",
      "Ep 6/500 | Reward: -699.90 | Eps: 0.94\n",
      "Ep 7/500 | Reward: -640.15 | Eps: 0.93\n",
      "Ep 8/500 | Reward: -600.45 | Eps: 0.92\n",
      "Ep 9/500 | Reward: -626.10 | Eps: 0.91\n",
      "Ep 10/500 | Reward: -607.85 | Eps: 0.90\n",
      "Ep 11/500 | Reward: -660.85 | Eps: 0.90\n",
      "Ep 12/500 | Reward: -558.25 | Eps: 0.89\n",
      "Ep 13/500 | Reward: -605.75 | Eps: 0.88\n",
      "Ep 14/500 | Reward: -804.25 | Eps: 0.87\n",
      "Ep 15/500 | Reward: -613.70 | Eps: 0.86\n",
      "Ep 16/500 | Reward: -628.00 | Eps: 0.85\n",
      "Ep 17/500 | Reward: -566.00 | Eps: 0.84\n",
      "Ep 18/500 | Reward: -630.45 | Eps: 0.83\n",
      "Ep 19/500 | Reward: -684.20 | Eps: 0.83\n",
      "Ep 20/500 | Reward: -604.15 | Eps: 0.82\n",
      "Ep 21/500 | Reward: -726.15 | Eps: 0.81\n",
      "Ep 22/500 | Reward: -697.35 | Eps: 0.80\n",
      "Ep 23/500 | Reward: -619.80 | Eps: 0.79\n",
      "Ep 24/500 | Reward: -637.65 | Eps: 0.79\n",
      "Ep 25/500 | Reward: -772.90 | Eps: 0.78\n",
      "Ep 26/500 | Reward: -665.70 | Eps: 0.77\n",
      "Ep 27/500 | Reward: -723.90 | Eps: 0.76\n",
      "Ep 28/500 | Reward: -641.85 | Eps: 0.75\n",
      "Ep 29/500 | Reward: -759.15 | Eps: 0.75\n",
      "Ep 30/500 | Reward: -786.50 | Eps: 0.74\n",
      "Ep 31/500 | Reward: -822.80 | Eps: 0.73\n",
      "Ep 32/500 | Reward: -757.90 | Eps: 0.72\n",
      "Ep 33/500 | Reward: -805.90 | Eps: 0.72\n",
      "Ep 34/500 | Reward: -730.25 | Eps: 0.71\n",
      "Ep 35/500 | Reward: -769.15 | Eps: 0.70\n",
      "Ep 36/500 | Reward: -913.15 | Eps: 0.70\n",
      "Ep 37/500 | Reward: -709.70 | Eps: 0.69\n",
      "Ep 38/500 | Reward: -762.70 | Eps: 0.68\n",
      "Ep 39/500 | Reward: -824.75 | Eps: 0.68\n",
      "Ep 40/500 | Reward: -807.65 | Eps: 0.67\n",
      "Ep 41/500 | Reward: -702.40 | Eps: 0.66\n",
      "Ep 42/500 | Reward: -872.40 | Eps: 0.66\n",
      "Ep 43/500 | Reward: -693.15 | Eps: 0.65\n",
      "Ep 44/500 | Reward: -730.45 | Eps: 0.64\n",
      "Ep 45/500 | Reward: -811.95 | Eps: 0.64\n",
      "Ep 46/500 | Reward: -751.55 | Eps: 0.63\n",
      "Ep 47/500 | Reward: -706.80 | Eps: 0.62\n",
      "Ep 48/500 | Reward: -716.20 | Eps: 0.62\n",
      "Ep 49/500 | Reward: -692.25 | Eps: 0.61\n",
      "Ep 50/500 | Reward: -741.70 | Eps: 0.61\n",
      "Ep 51/500 | Reward: -740.30 | Eps: 0.60\n",
      "Ep 52/500 | Reward: -774.95 | Eps: 0.59\n",
      "Ep 53/500 | Reward: -752.55 | Eps: 0.59\n",
      "Ep 54/500 | Reward: -772.65 | Eps: 0.58\n",
      "Ep 55/500 | Reward: -924.50 | Eps: 0.58\n",
      "Ep 56/500 | Reward: -720.25 | Eps: 0.57\n",
      "Ep 57/500 | Reward: -849.80 | Eps: 0.56\n",
      "Ep 58/500 | Reward: -874.35 | Eps: 0.56\n",
      "Ep 59/500 | Reward: -732.05 | Eps: 0.55\n",
      "Ep 60/500 | Reward: -810.55 | Eps: 0.55\n",
      "Ep 61/500 | Reward: -863.95 | Eps: 0.54\n",
      "Ep 62/500 | Reward: -825.05 | Eps: 0.54\n",
      "Ep 63/500 | Reward: -767.45 | Eps: 0.53\n",
      "Ep 64/500 | Reward: -786.50 | Eps: 0.53\n",
      "Ep 65/500 | Reward: -829.05 | Eps: 0.52\n",
      "Ep 66/500 | Reward: -875.75 | Eps: 0.52\n",
      "Ep 67/500 | Reward: -823.05 | Eps: 0.51\n",
      "Ep 68/500 | Reward: -825.30 | Eps: 0.50\n",
      "Ep 69/500 | Reward: -765.55 | Eps: 0.50\n",
      "Ep 70/500 | Reward: -882.95 | Eps: 0.49\n",
      "Ep 71/500 | Reward: -761.55 | Eps: 0.49\n",
      "Ep 72/500 | Reward: -824.10 | Eps: 0.48\n",
      "Ep 73/500 | Reward: -794.85 | Eps: 0.48\n",
      "Ep 74/500 | Reward: -956.60 | Eps: 0.48\n",
      "Ep 75/500 | Reward: -843.20 | Eps: 0.47\n",
      "Ep 76/500 | Reward: -828.35 | Eps: 0.47\n",
      "Ep 77/500 | Reward: -884.65 | Eps: 0.46\n",
      "Ep 78/500 | Reward: -807.20 | Eps: 0.46\n",
      "Ep 79/500 | Reward: -811.15 | Eps: 0.45\n",
      "Ep 80/500 | Reward: -695.65 | Eps: 0.45\n",
      "Ep 81/500 | Reward: -791.80 | Eps: 0.44\n",
      "Ep 82/500 | Reward: -840.55 | Eps: 0.44\n",
      "Ep 83/500 | Reward: -923.70 | Eps: 0.43\n",
      "Ep 84/500 | Reward: -972.95 | Eps: 0.43\n",
      "Ep 85/500 | Reward: -806.75 | Eps: 0.43\n",
      "Ep 86/500 | Reward: -844.30 | Eps: 0.42\n",
      "Ep 87/500 | Reward: -839.40 | Eps: 0.42\n",
      "Ep 88/500 | Reward: -658.65 | Eps: 0.41\n",
      "Ep 89/500 | Reward: -825.55 | Eps: 0.41\n",
      "Ep 90/500 | Reward: -802.95 | Eps: 0.40\n",
      "Ep 91/500 | Reward: -685.25 | Eps: 0.40\n",
      "Ep 92/500 | Reward: -861.25 | Eps: 0.40\n",
      "Ep 93/500 | Reward: -657.30 | Eps: 0.39\n",
      "Ep 94/500 | Reward: -650.25 | Eps: 0.39\n",
      "Ep 95/500 | Reward: -520.25 | Eps: 0.38\n",
      "Ep 96/500 | Reward: -584.85 | Eps: 0.38\n",
      "Ep 97/500 | Reward: -467.85 | Eps: 0.38\n",
      "Ep 98/500 | Reward: -467.10 | Eps: 0.37\n",
      "Ep 99/500 | Reward: -396.00 | Eps: 0.37\n",
      "Ep 100/500 | Reward: -481.20 | Eps: 0.37\n",
      "Ep 101/500 | Reward: -422.05 | Eps: 0.36\n",
      "Ep 102/500 | Reward: -295.45 | Eps: 0.36\n",
      "Ep 103/500 | Reward: -383.20 | Eps: 0.36\n",
      "Ep 104/500 | Reward: -337.30 | Eps: 0.35\n",
      "Ep 105/500 | Reward: -340.20 | Eps: 0.35\n",
      "Ep 106/500 | Reward: -383.80 | Eps: 0.34\n",
      "Ep 107/500 | Reward: -331.70 | Eps: 0.34\n",
      "Ep 108/500 | Reward: -239.65 | Eps: 0.34\n",
      "Ep 109/500 | Reward: -344.70 | Eps: 0.33\n",
      "Ep 110/500 | Reward: -266.10 | Eps: 0.33\n",
      "Ep 111/500 | Reward: -324.10 | Eps: 0.33\n",
      "Ep 112/500 | Reward: -345.95 | Eps: 0.32\n",
      "Ep 113/500 | Reward: -369.85 | Eps: 0.32\n",
      "Ep 114/500 | Reward: -328.00 | Eps: 0.32\n",
      "Ep 115/500 | Reward: -306.05 | Eps: 0.31\n",
      "Ep 116/500 | Reward: -337.50 | Eps: 0.31\n",
      "Ep 117/500 | Reward: -331.05 | Eps: 0.31\n",
      "Ep 118/500 | Reward: -290.20 | Eps: 0.31\n",
      "Ep 119/500 | Reward: -273.40 | Eps: 0.30\n",
      "Ep 120/500 | Reward: -259.50 | Eps: 0.30\n",
      "Ep 121/500 | Reward: -274.20 | Eps: 0.30\n",
      "Ep 122/500 | Reward: -306.05 | Eps: 0.29\n",
      "Ep 123/500 | Reward: -268.40 | Eps: 0.29\n",
      "Ep 124/500 | Reward: -300.20 | Eps: 0.29\n",
      "Ep 125/500 | Reward: -344.55 | Eps: 0.28\n",
      "Ep 126/500 | Reward: -251.20 | Eps: 0.28\n",
      "Ep 127/500 | Reward: -270.65 | Eps: 0.28\n",
      "Ep 128/500 | Reward: -302.60 | Eps: 0.28\n",
      "Ep 129/500 | Reward: -307.45 | Eps: 0.27\n",
      "Ep 130/500 | Reward: -247.25 | Eps: 0.27\n",
      "Ep 131/500 | Reward: -309.25 | Eps: 0.27\n",
      "Ep 132/500 | Reward: -290.65 | Eps: 0.27\n",
      "Ep 133/500 | Reward: -253.05 | Eps: 0.26\n",
      "Ep 134/500 | Reward: -346.80 | Eps: 0.26\n",
      "Ep 135/500 | Reward: -197.45 | Eps: 0.26\n",
      "Ep 136/500 | Reward: -341.40 | Eps: 0.25\n",
      "Ep 137/500 | Reward: -362.40 | Eps: 0.25\n",
      "Ep 138/500 | Reward: -272.15 | Eps: 0.25\n",
      "Ep 139/500 | Reward: -261.85 | Eps: 0.25\n",
      "Ep 140/500 | Reward: -388.40 | Eps: 0.24\n",
      "Ep 141/500 | Reward: -281.55 | Eps: 0.24\n",
      "Ep 142/500 | Reward: -333.65 | Eps: 0.24\n",
      "Ep 143/500 | Reward: -275.80 | Eps: 0.24\n",
      "Ep 144/500 | Reward: -326.30 | Eps: 0.24\n",
      "Ep 145/500 | Reward: -286.90 | Eps: 0.23\n",
      "Ep 146/500 | Reward: -232.40 | Eps: 0.23\n",
      "Ep 147/500 | Reward: -338.45 | Eps: 0.23\n",
      "Ep 148/500 | Reward: -272.65 | Eps: 0.23\n",
      "Ep 149/500 | Reward: -320.15 | Eps: 0.22\n",
      "Ep 150/500 | Reward: -388.25 | Eps: 0.22\n",
      "Ep 151/500 | Reward: -419.95 | Eps: 0.22\n",
      "Ep 152/500 | Reward: -270.15 | Eps: 0.22\n",
      "Ep 153/500 | Reward: -247.95 | Eps: 0.21\n",
      "Ep 154/500 | Reward: -265.90 | Eps: 0.21\n",
      "Ep 155/500 | Reward: -273.55 | Eps: 0.21\n",
      "Ep 156/500 | Reward: -274.95 | Eps: 0.21\n",
      "Ep 157/500 | Reward: -350.30 | Eps: 0.21\n",
      "Ep 158/500 | Reward: -315.75 | Eps: 0.20\n",
      "Ep 159/500 | Reward: -295.85 | Eps: 0.20\n",
      "Ep 160/500 | Reward: -243.10 | Eps: 0.20\n",
      "Ep 161/500 | Reward: -262.25 | Eps: 0.20\n",
      "Ep 162/500 | Reward: -342.65 | Eps: 0.20\n",
      "Ep 163/500 | Reward: -300.85 | Eps: 0.19\n",
      "Ep 164/500 | Reward: -378.65 | Eps: 0.19\n",
      "Ep 165/500 | Reward: -273.65 | Eps: 0.19\n",
      "Ep 166/500 | Reward: -347.65 | Eps: 0.19\n",
      "Ep 167/500 | Reward: -334.00 | Eps: 0.19\n",
      "Ep 168/500 | Reward: -295.30 | Eps: 0.18\n",
      "Ep 169/500 | Reward: -299.50 | Eps: 0.18\n",
      "Ep 170/500 | Reward: -317.65 | Eps: 0.18\n",
      "Ep 171/500 | Reward: -274.40 | Eps: 0.18\n",
      "Ep 172/500 | Reward: -259.45 | Eps: 0.18\n",
      "Ep 173/500 | Reward: -220.25 | Eps: 0.18\n",
      "Ep 174/500 | Reward: -272.70 | Eps: 0.17\n",
      "Ep 175/500 | Reward: -321.65 | Eps: 0.17\n",
      "Ep 176/500 | Reward: -265.45 | Eps: 0.17\n",
      "Ep 177/500 | Reward: -310.00 | Eps: 0.17\n",
      "Ep 178/500 | Reward: -280.95 | Eps: 0.17\n",
      "Ep 179/500 | Reward: -294.05 | Eps: 0.17\n",
      "Ep 180/500 | Reward: -297.65 | Eps: 0.16\n",
      "Ep 181/500 | Reward: -217.90 | Eps: 0.16\n",
      "Ep 182/500 | Reward: -252.20 | Eps: 0.16\n",
      "Ep 183/500 | Reward: -288.55 | Eps: 0.16\n",
      "Ep 184/500 | Reward: -299.35 | Eps: 0.16\n",
      "Ep 185/500 | Reward: -291.65 | Eps: 0.16\n",
      "Ep 186/500 | Reward: -266.80 | Eps: 0.15\n",
      "Ep 187/500 | Reward: -259.30 | Eps: 0.15\n",
      "Ep 188/500 | Reward: -277.35 | Eps: 0.15\n",
      "Ep 189/500 | Reward: -269.05 | Eps: 0.15\n",
      "Ep 190/500 | Reward: -290.85 | Eps: 0.15\n",
      "Ep 191/500 | Reward: -323.85 | Eps: 0.15\n",
      "Ep 192/500 | Reward: -293.35 | Eps: 0.15\n",
      "Ep 193/500 | Reward: -284.50 | Eps: 0.14\n",
      "Ep 194/500 | Reward: -305.15 | Eps: 0.14\n",
      "Ep 195/500 | Reward: -332.75 | Eps: 0.14\n",
      "Ep 196/500 | Reward: -332.90 | Eps: 0.14\n",
      "Ep 197/500 | Reward: -252.70 | Eps: 0.14\n",
      "Ep 198/500 | Reward: -282.35 | Eps: 0.14\n",
      "Ep 199/500 | Reward: -307.70 | Eps: 0.14\n",
      "Ep 200/500 | Reward: -381.55 | Eps: 0.13\n",
      "Ep 201/500 | Reward: -283.65 | Eps: 0.13\n",
      "Ep 202/500 | Reward: -202.90 | Eps: 0.13\n",
      "Ep 203/500 | Reward: -266.45 | Eps: 0.13\n",
      "Ep 204/500 | Reward: -291.90 | Eps: 0.13\n",
      "Ep 205/500 | Reward: -253.40 | Eps: 0.13\n",
      "Ep 206/500 | Reward: -350.40 | Eps: 0.13\n",
      "Ep 207/500 | Reward: -277.95 | Eps: 0.12\n",
      "Ep 208/500 | Reward: -240.10 | Eps: 0.12\n",
      "Ep 209/500 | Reward: -271.05 | Eps: 0.12\n",
      "Ep 210/500 | Reward: -233.25 | Eps: 0.12\n",
      "Ep 211/500 | Reward: -321.80 | Eps: 0.12\n",
      "Ep 212/500 | Reward: -255.55 | Eps: 0.12\n",
      "Ep 213/500 | Reward: -251.95 | Eps: 0.12\n",
      "Ep 214/500 | Reward: -292.20 | Eps: 0.12\n",
      "Ep 215/500 | Reward: -250.25 | Eps: 0.12\n",
      "Ep 216/500 | Reward: -261.55 | Eps: 0.11\n",
      "Ep 217/500 | Reward: -255.15 | Eps: 0.11\n",
      "Ep 218/500 | Reward: -199.65 | Eps: 0.11\n",
      "Ep 219/500 | Reward: -283.65 | Eps: 0.11\n",
      "Ep 220/500 | Reward: -281.25 | Eps: 0.11\n",
      "Ep 221/500 | Reward: -305.50 | Eps: 0.11\n",
      "Ep 222/500 | Reward: -286.55 | Eps: 0.11\n",
      "Ep 223/500 | Reward: -274.05 | Eps: 0.11\n",
      "Ep 224/500 | Reward: -210.85 | Eps: 0.11\n",
      "Ep 225/500 | Reward: -260.60 | Eps: 0.10\n",
      "Ep 226/500 | Reward: -272.40 | Eps: 0.10\n",
      "Ep 227/500 | Reward: -234.40 | Eps: 0.10\n",
      "Ep 228/500 | Reward: -270.25 | Eps: 0.10\n",
      "Ep 229/500 | Reward: -268.95 | Eps: 0.10\n",
      "Ep 230/500 | Reward: -340.30 | Eps: 0.10\n",
      "Ep 231/500 | Reward: -245.35 | Eps: 0.10\n",
      "Ep 232/500 | Reward: -268.05 | Eps: 0.10\n",
      "Ep 233/500 | Reward: -251.20 | Eps: 0.10\n",
      "Ep 234/500 | Reward: -282.50 | Eps: 0.10\n",
      "Ep 235/500 | Reward: -180.25 | Eps: 0.09\n",
      "Ep 236/500 | Reward: -257.60 | Eps: 0.09\n",
      "Ep 237/500 | Reward: -226.40 | Eps: 0.09\n",
      "Ep 238/500 | Reward: -307.90 | Eps: 0.09\n",
      "Ep 239/500 | Reward: -189.95 | Eps: 0.09\n",
      "Ep 240/500 | Reward: -289.95 | Eps: 0.09\n",
      "Ep 241/500 | Reward: -307.00 | Eps: 0.09\n",
      "Ep 242/500 | Reward: -210.90 | Eps: 0.09\n",
      "Ep 243/500 | Reward: -230.00 | Eps: 0.09\n",
      "Ep 244/500 | Reward: -270.90 | Eps: 0.09\n",
      "Ep 245/500 | Reward: -235.70 | Eps: 0.09\n",
      "Ep 246/500 | Reward: -250.75 | Eps: 0.08\n",
      "Ep 247/500 | Reward: -233.45 | Eps: 0.08\n",
      "Ep 248/500 | Reward: -212.10 | Eps: 0.08\n",
      "Ep 249/500 | Reward: -236.45 | Eps: 0.08\n",
      "Ep 250/500 | Reward: -239.90 | Eps: 0.08\n",
      "Ep 251/500 | Reward: -211.55 | Eps: 0.08\n",
      "Ep 252/500 | Reward: -261.10 | Eps: 0.08\n",
      "Ep 253/500 | Reward: -248.25 | Eps: 0.08\n",
      "Ep 254/500 | Reward: -219.20 | Eps: 0.08\n",
      "Ep 255/500 | Reward: -248.65 | Eps: 0.08\n",
      "Ep 256/500 | Reward: -268.55 | Eps: 0.08\n",
      "Ep 257/500 | Reward: -274.45 | Eps: 0.08\n",
      "Ep 258/500 | Reward: -273.20 | Eps: 0.07\n",
      "Ep 259/500 | Reward: -202.35 | Eps: 0.07\n",
      "Ep 260/500 | Reward: -252.50 | Eps: 0.07\n",
      "Ep 261/500 | Reward: -240.85 | Eps: 0.07\n",
      "Ep 262/500 | Reward: -266.10 | Eps: 0.07\n",
      "Ep 263/500 | Reward: -244.65 | Eps: 0.07\n",
      "Ep 264/500 | Reward: -266.20 | Eps: 0.07\n",
      "Ep 265/500 | Reward: -297.85 | Eps: 0.07\n",
      "Ep 266/500 | Reward: -345.40 | Eps: 0.07\n",
      "Ep 267/500 | Reward: -365.20 | Eps: 0.07\n",
      "Ep 268/500 | Reward: -261.45 | Eps: 0.07\n",
      "Ep 269/500 | Reward: -285.50 | Eps: 0.07\n",
      "Ep 270/500 | Reward: -236.35 | Eps: 0.07\n",
      "Ep 271/500 | Reward: -264.00 | Eps: 0.07\n",
      "Ep 272/500 | Reward: -284.10 | Eps: 0.06\n",
      "Ep 273/500 | Reward: -310.05 | Eps: 0.06\n",
      "Ep 274/500 | Reward: -285.75 | Eps: 0.06\n",
      "Ep 275/500 | Reward: -209.80 | Eps: 0.06\n",
      "Ep 276/500 | Reward: -246.10 | Eps: 0.06\n",
      "Ep 277/500 | Reward: -328.40 | Eps: 0.06\n",
      "Ep 278/500 | Reward: -244.65 | Eps: 0.06\n",
      "Ep 279/500 | Reward: -259.70 | Eps: 0.06\n",
      "Ep 280/500 | Reward: -262.95 | Eps: 0.06\n",
      "Ep 281/500 | Reward: -291.90 | Eps: 0.06\n",
      "Ep 282/500 | Reward: -228.10 | Eps: 0.06\n",
      "Ep 283/500 | Reward: -240.40 | Eps: 0.06\n",
      "Ep 284/500 | Reward: -277.45 | Eps: 0.06\n",
      "Ep 285/500 | Reward: -235.75 | Eps: 0.06\n",
      "Ep 286/500 | Reward: -223.45 | Eps: 0.06\n",
      "Ep 287/500 | Reward: -239.55 | Eps: 0.06\n",
      "Ep 288/500 | Reward: -301.65 | Eps: 0.06\n",
      "Ep 289/500 | Reward: -197.85 | Eps: 0.05\n",
      "Ep 290/500 | Reward: -263.30 | Eps: 0.05\n",
      "Ep 291/500 | Reward: -233.85 | Eps: 0.05\n",
      "Ep 292/500 | Reward: -285.60 | Eps: 0.05\n",
      "Ep 293/500 | Reward: -274.65 | Eps: 0.05\n",
      "Ep 294/500 | Reward: -216.75 | Eps: 0.05\n",
      "Ep 295/500 | Reward: -157.85 | Eps: 0.05\n",
      "Ep 296/500 | Reward: -205.95 | Eps: 0.05\n",
      "Ep 297/500 | Reward: -205.30 | Eps: 0.05\n",
      "Ep 298/500 | Reward: -253.10 | Eps: 0.05\n",
      "Ep 299/500 | Reward: -308.30 | Eps: 0.05\n",
      "Ep 300/500 | Reward: -197.40 | Eps: 0.05\n",
      "Ep 301/500 | Reward: -255.75 | Eps: 0.05\n",
      "Ep 302/500 | Reward: -245.50 | Eps: 0.05\n",
      "Ep 303/500 | Reward: -236.65 | Eps: 0.05\n",
      "Ep 304/500 | Reward: -274.15 | Eps: 0.05\n",
      "Ep 305/500 | Reward: -244.30 | Eps: 0.05\n",
      "Ep 306/500 | Reward: -255.55 | Eps: 0.05\n",
      "Ep 307/500 | Reward: -208.50 | Eps: 0.05\n",
      "Ep 308/500 | Reward: -289.30 | Eps: 0.05\n",
      "Ep 309/500 | Reward: -506.55 | Eps: 0.04\n",
      "Ep 310/500 | Reward: -201.40 | Eps: 0.04\n",
      "Ep 311/500 | Reward: -240.55 | Eps: 0.04\n",
      "Ep 312/500 | Reward: -243.70 | Eps: 0.04\n",
      "Ep 313/500 | Reward: -200.80 | Eps: 0.04\n",
      "Ep 314/500 | Reward: -268.60 | Eps: 0.04\n",
      "Ep 315/500 | Reward: -277.70 | Eps: 0.04\n",
      "Ep 316/500 | Reward: -272.85 | Eps: 0.04\n",
      "Ep 317/500 | Reward: -233.75 | Eps: 0.04\n",
      "Ep 318/500 | Reward: -252.10 | Eps: 0.04\n",
      "Ep 319/500 | Reward: -277.75 | Eps: 0.04\n",
      "Ep 320/500 | Reward: -237.75 | Eps: 0.04\n",
      "Ep 321/500 | Reward: -193.75 | Eps: 0.04\n",
      "Ep 322/500 | Reward: -289.25 | Eps: 0.04\n",
      "Ep 323/500 | Reward: -246.90 | Eps: 0.04\n",
      "Ep 324/500 | Reward: -215.45 | Eps: 0.04\n",
      "Ep 325/500 | Reward: -327.70 | Eps: 0.04\n",
      "Ep 326/500 | Reward: -264.50 | Eps: 0.04\n",
      "Ep 327/500 | Reward: -278.95 | Eps: 0.04\n",
      "Ep 328/500 | Reward: -328.95 | Eps: 0.04\n",
      "Ep 329/500 | Reward: -198.45 | Eps: 0.04\n",
      "Ep 330/500 | Reward: -268.50 | Eps: 0.04\n",
      "Ep 331/500 | Reward: -214.45 | Eps: 0.04\n",
      "Ep 332/500 | Reward: -195.30 | Eps: 0.04\n",
      "Ep 333/500 | Reward: -254.60 | Eps: 0.04\n",
      "Ep 334/500 | Reward: -216.05 | Eps: 0.03\n",
      "Ep 335/500 | Reward: -180.35 | Eps: 0.03\n",
      "Ep 336/500 | Reward: -213.50 | Eps: 0.03\n",
      "Ep 337/500 | Reward: -374.65 | Eps: 0.03\n",
      "Ep 338/500 | Reward: -242.55 | Eps: 0.03\n",
      "Ep 339/500 | Reward: -294.10 | Eps: 0.03\n",
      "Ep 340/500 | Reward: -213.60 | Eps: 0.03\n",
      "Ep 341/500 | Reward: -194.30 | Eps: 0.03\n",
      "Ep 342/500 | Reward: -269.75 | Eps: 0.03\n",
      "Ep 343/500 | Reward: -284.10 | Eps: 0.03\n",
      "Ep 344/500 | Reward: -248.10 | Eps: 0.03\n",
      "Ep 345/500 | Reward: -326.85 | Eps: 0.03\n",
      "Ep 346/500 | Reward: -432.20 | Eps: 0.03\n",
      "Ep 347/500 | Reward: -252.50 | Eps: 0.03\n",
      "Ep 348/500 | Reward: -248.25 | Eps: 0.03\n",
      "Ep 349/500 | Reward: -153.85 | Eps: 0.03\n",
      "Ep 350/500 | Reward: -135.70 | Eps: 0.03\n",
      "Ep 351/500 | Reward: -215.65 | Eps: 0.03\n",
      "Ep 352/500 | Reward: -230.40 | Eps: 0.03\n",
      "Ep 353/500 | Reward: -279.35 | Eps: 0.03\n",
      "Ep 354/500 | Reward: -176.55 | Eps: 0.03\n",
      "Ep 355/500 | Reward: -233.80 | Eps: 0.03\n",
      "Ep 356/500 | Reward: -193.05 | Eps: 0.03\n",
      "Ep 357/500 | Reward: -285.75 | Eps: 0.03\n",
      "Ep 358/500 | Reward: -200.30 | Eps: 0.03\n",
      "Ep 359/500 | Reward: -209.10 | Eps: 0.03\n",
      "Ep 360/500 | Reward: -198.80 | Eps: 0.03\n",
      "Ep 361/500 | Reward: -203.35 | Eps: 0.03\n",
      "Ep 362/500 | Reward: -274.25 | Eps: 0.03\n",
      "Ep 363/500 | Reward: -244.50 | Eps: 0.03\n",
      "Ep 364/500 | Reward: -265.30 | Eps: 0.03\n",
      "Ep 365/500 | Reward: -222.70 | Eps: 0.03\n",
      "Ep 366/500 | Reward: -252.20 | Eps: 0.03\n",
      "Ep 367/500 | Reward: -288.25 | Eps: 0.03\n",
      "Ep 368/500 | Reward: -233.45 | Eps: 0.02\n",
      "Ep 369/500 | Reward: -224.75 | Eps: 0.02\n",
      "Ep 370/500 | Reward: -308.55 | Eps: 0.02\n",
      "Ep 371/500 | Reward: -254.95 | Eps: 0.02\n",
      "Ep 372/500 | Reward: -170.75 | Eps: 0.02\n",
      "Ep 373/500 | Reward: -276.20 | Eps: 0.02\n",
      "Ep 374/500 | Reward: -232.20 | Eps: 0.02\n",
      "Ep 375/500 | Reward: -394.65 | Eps: 0.02\n",
      "Ep 376/500 | Reward: -226.50 | Eps: 0.02\n",
      "Ep 377/500 | Reward: -196.80 | Eps: 0.02\n",
      "Ep 378/500 | Reward: -244.35 | Eps: 0.02\n",
      "Ep 379/500 | Reward: -276.10 | Eps: 0.02\n",
      "Ep 380/500 | Reward: -216.20 | Eps: 0.02\n",
      "Ep 381/500 | Reward: -135.70 | Eps: 0.02\n",
      "Ep 382/500 | Reward: -172.15 | Eps: 0.02\n",
      "Ep 383/500 | Reward: -300.60 | Eps: 0.02\n",
      "Ep 384/500 | Reward: -240.15 | Eps: 0.02\n",
      "Ep 385/500 | Reward: -233.80 | Eps: 0.02\n",
      "Ep 386/500 | Reward: -248.55 | Eps: 0.02\n",
      "Ep 387/500 | Reward: -271.20 | Eps: 0.02\n",
      "Ep 388/500 | Reward: -259.65 | Eps: 0.02\n",
      "Ep 389/500 | Reward: -231.25 | Eps: 0.02\n",
      "Ep 390/500 | Reward: -135.70 | Eps: 0.02\n",
      "Ep 391/500 | Reward: -217.50 | Eps: 0.02\n",
      "Ep 392/500 | Reward: -192.75 | Eps: 0.02\n",
      "Ep 393/500 | Reward: -230.80 | Eps: 0.02\n",
      "Ep 394/500 | Reward: -166.55 | Eps: 0.02\n",
      "Ep 395/500 | Reward: -301.90 | Eps: 0.02\n",
      "Ep 396/500 | Reward: -166.70 | Eps: 0.02\n",
      "Ep 397/500 | Reward: -263.80 | Eps: 0.02\n",
      "Ep 398/500 | Reward: -244.75 | Eps: 0.02\n",
      "Ep 399/500 | Reward: -259.60 | Eps: 0.02\n",
      "Ep 400/500 | Reward: -243.60 | Eps: 0.02\n",
      "Ep 401/500 | Reward: -293.05 | Eps: 0.02\n",
      "Ep 402/500 | Reward: -218.25 | Eps: 0.02\n",
      "Ep 403/500 | Reward: -276.65 | Eps: 0.02\n",
      "Ep 404/500 | Reward: -135.70 | Eps: 0.02\n",
      "Ep 405/500 | Reward: -241.95 | Eps: 0.02\n",
      "Ep 406/500 | Reward: -212.95 | Eps: 0.02\n",
      "Ep 407/500 | Reward: -263.70 | Eps: 0.02\n",
      "Ep 408/500 | Reward: -223.90 | Eps: 0.02\n",
      "Ep 409/500 | Reward: -210.70 | Eps: 0.02\n",
      "Ep 410/500 | Reward: -198.05 | Eps: 0.02\n",
      "Ep 411/500 | Reward: -276.10 | Eps: 0.02\n",
      "Ep 412/500 | Reward: -230.40 | Eps: 0.02\n",
      "Ep 413/500 | Reward: -135.70 | Eps: 0.02\n",
      "Ep 414/500 | Reward: -234.45 | Eps: 0.02\n",
      "Ep 415/500 | Reward: -240.50 | Eps: 0.02\n",
      "Ep 416/500 | Reward: -284.40 | Eps: 0.02\n",
      "Ep 417/500 | Reward: -269.05 | Eps: 0.02\n",
      "Ep 418/500 | Reward: -216.65 | Eps: 0.01\n",
      "Ep 419/500 | Reward: -220.60 | Eps: 0.01\n",
      "Ep 420/500 | Reward: -164.35 | Eps: 0.01\n",
      "Ep 421/500 | Reward: -177.95 | Eps: 0.01\n",
      "Ep 422/500 | Reward: -193.15 | Eps: 0.01\n",
      "Ep 423/500 | Reward: -274.40 | Eps: 0.01\n",
      "Ep 424/500 | Reward: -265.50 | Eps: 0.01\n",
      "Ep 425/500 | Reward: -245.40 | Eps: 0.01\n",
      "Ep 426/500 | Reward: -281.65 | Eps: 0.01\n",
      "Ep 427/500 | Reward: -251.60 | Eps: 0.01\n",
      "Ep 428/500 | Reward: -199.95 | Eps: 0.01\n",
      "Ep 429/500 | Reward: -226.15 | Eps: 0.01\n",
      "Ep 430/500 | Reward: -135.70 | Eps: 0.01\n",
      "Ep 431/500 | Reward: -170.55 | Eps: 0.01\n",
      "Ep 432/500 | Reward: -276.00 | Eps: 0.01\n",
      "Ep 433/500 | Reward: -229.05 | Eps: 0.01\n",
      "Ep 434/500 | Reward: -165.55 | Eps: 0.01\n",
      "Ep 435/500 | Reward: -214.00 | Eps: 0.01\n",
      "Ep 436/500 | Reward: -214.60 | Eps: 0.01\n",
      "Ep 437/500 | Reward: -135.70 | Eps: 0.01\n",
      "Ep 438/500 | Reward: -215.25 | Eps: 0.01\n",
      "Ep 439/500 | Reward: -177.60 | Eps: 0.01\n",
      "Ep 440/500 | Reward: -135.70 | Eps: 0.01\n",
      "Ep 441/500 | Reward: -214.40 | Eps: 0.01\n",
      "Ep 442/500 | Reward: -135.70 | Eps: 0.01\n",
      "Ep 443/500 | Reward: -153.80 | Eps: 0.01\n",
      "Ep 444/500 | Reward: -322.65 | Eps: 0.01\n",
      "Ep 445/500 | Reward: -218.70 | Eps: 0.01\n",
      "Ep 446/500 | Reward: -288.80 | Eps: 0.01\n",
      "Ep 447/500 | Reward: -243.95 | Eps: 0.01\n",
      "Ep 448/500 | Reward: -242.20 | Eps: 0.01\n",
      "Ep 449/500 | Reward: -213.70 | Eps: 0.01\n",
      "Ep 450/500 | Reward: -231.25 | Eps: 0.01\n",
      "Ep 451/500 | Reward: -237.00 | Eps: 0.01\n",
      "Ep 452/500 | Reward: -271.05 | Eps: 0.01\n",
      "Ep 453/500 | Reward: -230.70 | Eps: 0.01\n",
      "Ep 454/500 | Reward: -250.40 | Eps: 0.01\n",
      "Ep 455/500 | Reward: -224.90 | Eps: 0.01\n",
      "Ep 456/500 | Reward: -191.95 | Eps: 0.01\n",
      "Ep 457/500 | Reward: -105.30 | Eps: 0.01\n",
      "Ep 458/500 | Reward: -177.30 | Eps: 0.01\n",
      "Ep 459/500 | Reward: -135.70 | Eps: 0.01\n",
      "Ep 460/500 | Reward: -173.20 | Eps: 0.01\n",
      "Ep 461/500 | Reward: -251.50 | Eps: 0.01\n",
      "Ep 462/500 | Reward: -210.85 | Eps: 0.01\n",
      "Ep 463/500 | Reward: -235.65 | Eps: 0.01\n",
      "Ep 464/500 | Reward: -151.05 | Eps: 0.01\n",
      "Ep 465/500 | Reward: -236.95 | Eps: 0.01\n",
      "Ep 466/500 | Reward: -133.75 | Eps: 0.01\n",
      "Ep 467/500 | Reward: -186.95 | Eps: 0.01\n",
      "Ep 468/500 | Reward: -212.45 | Eps: 0.01\n",
      "Ep 469/500 | Reward: -135.70 | Eps: 0.01\n",
      "Ep 470/500 | Reward: -173.20 | Eps: 0.01\n",
      "Ep 471/500 | Reward: -280.35 | Eps: 0.01\n",
      "Ep 472/500 | Reward: -289.50 | Eps: 0.01\n",
      "Ep 473/500 | Reward: -230.00 | Eps: 0.01\n",
      "Ep 474/500 | Reward: -135.70 | Eps: 0.01\n",
      "Ep 475/500 | Reward: -150.60 | Eps: 0.01\n",
      "Ep 476/500 | Reward: -241.20 | Eps: 0.01\n",
      "Ep 477/500 | Reward: -218.90 | Eps: 0.01\n",
      "Ep 478/500 | Reward: -309.60 | Eps: 0.01\n",
      "Ep 479/500 | Reward: -186.85 | Eps: 0.01\n",
      "Ep 480/500 | Reward: -274.25 | Eps: 0.01\n",
      "Ep 481/500 | Reward: -169.95 | Eps: 0.01\n",
      "Ep 482/500 | Reward: -223.85 | Eps: 0.01\n",
      "Ep 483/500 | Reward: -275.45 | Eps: 0.01\n",
      "Ep 484/500 | Reward: -255.50 | Eps: 0.01\n",
      "Ep 485/500 | Reward: -282.25 | Eps: 0.01\n",
      "Ep 486/500 | Reward: -160.10 | Eps: 0.01\n",
      "Ep 487/500 | Reward: -165.75 | Eps: 0.01\n",
      "Ep 488/500 | Reward: -146.90 | Eps: 0.01\n",
      "Ep 489/500 | Reward: -135.70 | Eps: 0.01\n",
      "Ep 490/500 | Reward: -241.05 | Eps: 0.01\n",
      "Ep 491/500 | Reward: -216.35 | Eps: 0.01\n",
      "Ep 492/500 | Reward: -218.30 | Eps: 0.01\n",
      "Ep 493/500 | Reward: -220.55 | Eps: 0.01\n",
      "Ep 494/500 | Reward: -260.30 | Eps: 0.01\n",
      "Ep 495/500 | Reward: -252.50 | Eps: 0.01\n",
      "Ep 496/500 | Reward: -135.70 | Eps: 0.01\n",
      "Ep 497/500 | Reward: -222.50 | Eps: 0.01\n",
      "Ep 498/500 | Reward: -209.15 | Eps: 0.01\n",
      "Ep 499/500 | Reward: -225.15 | Eps: 0.01\n",
      "Ep 500/500 | Reward: -135.70 | Eps: 0.01\n",
      "Training Completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION (OPTIMIZED FOR 1 HOUR)\n",
    "# ==========================================\n",
    "# Paths\n",
    "BASE_DIR = os.getcwd()\n",
    "SCENARIO_DIR = os.path.join(BASE_DIR, \"Network with RL control\")\n",
    "RESULTS_DIR = os.path.join(SCENARIO_DIR, \"results\")\n",
    "MODEL_DIR = os.path.join(SCENARIO_DIR, \"models\")\n",
    "CONFIG_PATH = os.path.join(SCENARIO_DIR, \"ff_heterogeneous.sumocfg\")\n",
    "\n",
    "# Hyperparameters\n",
    "NUM_EPISODES = 500          # Fits within 1 hour\n",
    "MAX_STEPS = 900             # 15 minutes simulated per episode (enough to learn congestion)\n",
    "BATCH_SIZE = 32             # Faster processing\n",
    "MEMORY_SIZE = 20000\n",
    "GAMMA = 0.95                # Discount factor\n",
    "LEARNING_RATE = 0.002       # Slightly higher learning rate for faster convergence\n",
    "\n",
    "# Exploration Settings\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.01\n",
    "EPSILON_DECAY = 0.99        # Reaches minimal exploration around episode 300\n",
    "\n",
    "# Traffic Settings\n",
    "TLS_IDS = [\"E1\", \"E2\", \"E3\", \"E4\"]\n",
    "YELLOW_DURATION = 3\n",
    "MIN_GREEN = 10\n",
    "\n",
    "# Hardware Check\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"--- TRAINING CONFIGURATION ---\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Target Time: ~1 Hour\")\n",
    "print(f\"Episodes: {NUM_EPISODES}\")\n",
    "print(f\"Steps per Episode: {MAX_STEPS}\")\n",
    "print(\"------------------------------\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. NEURAL NETWORK (DQN)\n",
    "# ==========================================\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DQN, self).__init__()\n",
    "        # Lighter network for faster training on normal PC\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# ==========================================\n",
    "# 3. RL AGENT\n",
    "# ==========================================\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
    "        self.epsilon = EPSILON_START\n",
    "        \n",
    "        self.policy_net = DQN(state_size, action_size).to(device)\n",
    "        self.target_net = DQN(state_size, action_size).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=LEARNING_RATE)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def act(self, state):\n",
    "        if random.random() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.policy_net(state_tensor)\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        minibatch = random.sample(self.memory, BATCH_SIZE)\n",
    "        \n",
    "        states = torch.FloatTensor(np.array([m[0] for m in minibatch])).to(device)\n",
    "        actions = torch.LongTensor([m[1] for m in minibatch]).unsqueeze(1).to(device)\n",
    "        rewards = torch.FloatTensor([m[2] for m in minibatch]).to(device)\n",
    "        next_states = torch.FloatTensor(np.array([m[3] for m in minibatch])).to(device)\n",
    "        dones = torch.FloatTensor([m[4] for m in minibatch]).to(device)\n",
    "\n",
    "        curr_Q = self.policy_net(states).gather(1, actions).squeeze(1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_Q = self.target_net(next_states).max(1)[0]\n",
    "            target_Q = rewards + (GAMMA * next_Q * (1 - dones))\n",
    "\n",
    "        loss = self.criterion(curr_Q, target_Q)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save(self.policy_net.state_dict(), path)\n",
    "\n",
    "# ==========================================\n",
    "# 4. SUMO HELPER FUNCTIONS\n",
    "# ==========================================\n",
    "def check_sumo_env():\n",
    "    if 'SUMO_HOME' not in os.environ:\n",
    "        sys.exit(\"Error: SUMO_HOME environment variable is not set.\")\n",
    "    tools = os.path.join(os.environ['SUMO_HOME'], 'tools')\n",
    "    sys.path.append(tools)\n",
    "    os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "    \n",
    "    # Ensure config file exists\n",
    "    if not os.path.exists(CONFIG_PATH):\n",
    "        import shutil\n",
    "        print(\"Config file not found, attempting to copy from 'Original network'...\")\n",
    "        original_dir = os.path.join(BASE_DIR, \"Original network\")\n",
    "        for file in [\"ff_heterogeneous.sumocfg\", \"ff.net.xml\", \"ff_heterogeneous.rou.xml\"]:\n",
    "            src = os.path.join(original_dir, file)\n",
    "            dst = os.path.join(SCENARIO_DIR, file)\n",
    "            if os.path.exists(src):\n",
    "                shutil.copy(src, dst)\n",
    "    return tools\n",
    "\n",
    "tools_path = check_sumo_env()\n",
    "import traci\n",
    "\n",
    "def get_sumo_binary():\n",
    "    # Always headless for performance\n",
    "    return os.path.join(os.environ['SUMO_HOME'], 'bin', 'sumo') \n",
    "\n",
    "def get_state(tls_id):\n",
    "    \"\"\"\n",
    "    State (Size 3):\n",
    "    1. Total Queue Length (Normalized)\n",
    "    2. Max Queue Length (Normalized)\n",
    "    3. Current Phase Index (Normalized)\n",
    "    \"\"\"\n",
    "    lanes = traci.trafficlight.getControlledLanes(tls_id)\n",
    "    halting = [traci.lane.getLastStepHaltingNumber(lane) for lane in lanes]\n",
    "    \n",
    "    total_queue = sum(halting) / 50.0  # Normalized\n",
    "    max_queue = max(halting) / 20.0    # Normalized\n",
    "    phase = traci.trafficlight.getPhase(tls_id) / 4.0\n",
    "    \n",
    "    return np.array([total_queue, max_queue, phase], dtype=np.float32)\n",
    "\n",
    "def get_reward(tls_id, waiting_time_prev):\n",
    "    \"\"\"\n",
    "    Reward function CLIPPED to prevent huge numbers.\n",
    "    \"\"\"\n",
    "    lanes = traci.trafficlight.getControlledLanes(tls_id)\n",
    "    current_wait = sum([traci.lane.getWaitingTime(lane) for lane in lanes])\n",
    "    current_queue = sum([traci.lane.getLastStepHaltingNumber(lane) for lane in lanes])\n",
    "    \n",
    "    # Reward = (WaitTime Reduction) - (Queue Penalty)\n",
    "    diff = waiting_time_prev - current_wait\n",
    "    \n",
    "    # Scaling to keep reward small (e.g., between -5 and 5)\n",
    "    reward = (diff * 0.2) - (current_queue * 0.05)\n",
    "    \n",
    "    # CLAMPING: Crucial for stability\n",
    "    reward = max(min(reward, 5.0), -5.0)\n",
    "    \n",
    "    return reward, current_wait\n",
    "\n",
    "# ==========================================\n",
    "# 5. MAIN TRAINING LOOP\n",
    "# ==========================================\n",
    "def train():\n",
    "    # Input: 3, Output: 2\n",
    "    agent = DQNAgent(state_size=3, action_size=2)\n",
    "    sumo_bin = get_sumo_binary()\n",
    "    \n",
    "    print(\"\\nStarting Optimized Training...\")\n",
    "\n",
    "    for episode in range(NUM_EPISODES):\n",
    "        # Start SUMO\n",
    "        sumo_cmd = [sumo_bin, \"-c\", CONFIG_PATH, \"--no-step-log\", \"true\", \"--waiting-time-memory\", \"1000\"]\n",
    "        try:\n",
    "            traci.start(sumo_cmd)\n",
    "        except Exception as e:\n",
    "            print(f\"Error starting SUMO: {e}\")\n",
    "            break\n",
    "\n",
    "        step = 0\n",
    "        total_reward = 0\n",
    "        \n",
    "        # Tracking data for all 4 intersections\n",
    "        tls_data = {t: {\"prev_wait\": 0, \"phase_time\": 0} for t in TLS_IDS}\n",
    "\n",
    "        while step < MAX_STEPS:\n",
    "            traci.simulationStep()\n",
    "            step += 1\n",
    "            \n",
    "            # Make decisions every 5 seconds (not every step)\n",
    "            if step % 5 == 0:\n",
    "                for tls in TLS_IDS:\n",
    "                    # 1. Observe State\n",
    "                    state = get_state(tls)\n",
    "                    \n",
    "                    # 2. Select Action\n",
    "                    action = agent.act(state)\n",
    "                    \n",
    "                    # 3. Apply Action (Logic)\n",
    "                    current_phase = traci.trafficlight.getPhase(tls)\n",
    "                    \n",
    "                    # Phase 0=NS Green, 1=Yellow, 2=EW Green, 3=Yellow\n",
    "                    # Action 0=NS Green, Action 1=EW Green\n",
    "                    \n",
    "                    if current_phase == 1 or current_phase == 3:\n",
    "                        # Yellow phase logic: wait it out\n",
    "                        tls_data[tls][\"phase_time\"] += 5\n",
    "                        if tls_data[tls][\"phase_time\"] >= YELLOW_DURATION:\n",
    "                            next_phase = 2 if current_phase == 1 else 0\n",
    "                            traci.trafficlight.setPhase(tls, next_phase)\n",
    "                            tls_data[tls][\"phase_time\"] = 0\n",
    "                    else:\n",
    "                        # Green phase logic\n",
    "                        target_phase = 0 if action == 0 else 2\n",
    "                        \n",
    "                        if current_phase != target_phase:\n",
    "                            # We want to switch\n",
    "                            if tls_data[tls][\"phase_time\"] >= MIN_GREEN:\n",
    "                                traci.trafficlight.setPhase(tls, current_phase + 1) # Go Yellow\n",
    "                                tls_data[tls][\"phase_time\"] = 0\n",
    "                            else:\n",
    "                                tls_data[tls][\"phase_time\"] += 5 # Keep Green\n",
    "                        else:\n",
    "                            tls_data[tls][\"phase_time\"] += 5 # Keep Green\n",
    "                    \n",
    "                    # 4. Reward & Learn\n",
    "                    reward, new_wait = get_reward(tls, tls_data[tls][\"prev_wait\"])\n",
    "                    \n",
    "                    # Remember\n",
    "                    next_state = get_state(tls)\n",
    "                    agent.remember(state, action, reward, next_state, False)\n",
    "                    \n",
    "                    tls_data[tls][\"prev_wait\"] = new_wait\n",
    "                    total_reward += reward\n",
    "\n",
    "                # Train Agent\n",
    "                agent.replay()\n",
    "\n",
    "        traci.close()\n",
    "        \n",
    "        # Update Target Net\n",
    "        if episode % 5 == 0:\n",
    "            agent.update_target_network()\n",
    "            \n",
    "        # Decay Epsilon\n",
    "        if agent.epsilon > EPSILON_END:\n",
    "            agent.epsilon *= EPSILON_DECAY\n",
    "\n",
    "        print(f\"Ep {episode+1}/{NUM_EPISODES} | Reward: {total_reward:.2f} | Eps: {agent.epsilon:.2f}\")\n",
    "\n",
    "        # Autosave\n",
    "        if (episode + 1) % 25 == 0:\n",
    "            agent.save(os.path.join(MODEL_DIR, f\"model_ep{episode+1}.pth\"))\n",
    "\n",
    "    agent.save(os.path.join(MODEL_DIR, \"final_model.pth\"))\n",
    "    print(\"Training Completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e36c51b",
   "metadata": {},
   "source": [
    "time 1h30"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icvis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
